{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from xml.etree.ElementTree import fromstring \n",
    "import shutil\n",
    "import warnings\n",
    "import random\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove old data and database\n",
    "\n",
    "if os.path.exists(\"data\"):\n",
    "    shutil.rmtree(\"data\")\n",
    "os.makedirs(\"data\")\n",
    "\n",
    "# # Create required directories\n",
    "# directories = ['./data/fighters/', './data/git/']\n",
    "# for directory in directories:\n",
    "#     os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# # Define CSV files and their columns\n",
    "# csv_files = {\n",
    "#     './data/event_urls_sherdog.csv': ['Event_URL'],\n",
    "#     './data/fighter_id_sherdog.csv': ['Fighter', 'Fighter_ID'],\n",
    "#     './data/event_data_sherdog.csv': ['Event Name', 'Event Location', 'Event Date', \n",
    "#                                       'Fighter 1', 'Fighter 2', 'Weight Class', \n",
    "#                                       'Winning Fighter', 'Winning Method', \n",
    "#                                       'Winning Round', 'Winning Time', 'Referee'],\n",
    "#     './data/fighter_info.csv': ['Fighter', 'Nickname', 'Birth Date', 'Nationality', \n",
    "#                                 'Hometown', 'Association', 'Weight Class', 'Height', \n",
    "#                                 'REACH', 'STANCE', 'Wins', 'Losses', 'Win_Decision', \n",
    "#                                 'Win_KO', 'Win_Sub', 'Loss_Decision', 'Loss_KO', \n",
    "#                                 'Loss_Sub', 'Sherdog URL', 'BFO URL']\n",
    "# }\n",
    "\n",
    "# # Ensure CSV files exist and have headers\n",
    "# for file_path, columns in csv_files.items():\n",
    "#     if not os.path.exists(file_path):\n",
    "#         pd.DataFrame(columns=columns).to_csv(file_path, index=False)\n",
    "\n",
    "# # Function to download GitHub data\n",
    "# def download_github_files():\n",
    "#     files = [\"ufc_event_details.csv\", \"ufc_fight_results.csv\", \n",
    "#              \"ufc_fight_stats.csv\", \"ufc_fighter_tott.csv\"]\n",
    "#     base_url = \"https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/\"\n",
    "#     save_path = \"./data/git/\"\n",
    "\n",
    "#     for file in files:\n",
    "#         response = requests.get(base_url + file)\n",
    "#         if response.status_code == 200:\n",
    "#             with open(os.path.join(save_path, file), 'wb') as f:\n",
    "#                 f.write(response.content)\n",
    "#         else:\n",
    "#             print(f\"Failed to download {file}\")\n",
    "\n",
    "# # Execute file download\n",
    "# download_github_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sherdog\n",
    "##### [sherdog.com](https://www.sherdog.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sherdog: Event URLs\n",
    "# event_urls_sherdog.csv\n",
    "\n",
    "urls = [\n",
    "    'https://www.sherdog.com/organizations/Ultimate-Fighting-Championship-UFC-2/recent-events/1',\n",
    "    'https://www.sherdog.com/organizations/Ultimate-Fighting-Championship-UFC-2/recent-events/2',\n",
    "    'https://www.sherdog.com/organizations/Ultimate-Fighting-Championship-UFC-2/recent-events/3',\n",
    "    'https://www.sherdog.com/organizations/Ultimate-Fighting-Championship-UFC-2/recent-events/4',\n",
    "    'https://www.sherdog.com/organizations/Ultimate-Fighting-Championship-UFC-2/recent-events/5',\n",
    "    'https://www.sherdog.com/organizations/Ultimate-Fighting-Championship-UFC-2/recent-events/6',\n",
    "    'https://www.sherdog.com/organizations/Ultimate-Fighting-Championship-UFC-2/recent-events/7',\n",
    "    'https://www.sherdog.com/organizations/Ultimate-Fighting-Championship-UFC-2/recent-events/8'\n",
    "]\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0\"\n",
    "]\n",
    "headers = {\"User-Agent\": random.choice(user_agents)}\n",
    "df = pd.read_csv('./data/event_urls_sherdog.csv') if os.path.isfile('./data/event_urls_sherdog.csv') else pd.DataFrame(columns=['Event_URL'])\n",
    "existing_urls = set(df['Event_URL'])\n",
    "\n",
    "print(f\"Starting to scrape {len(urls)} URLs...\")\n",
    "for i, url in enumerate(urls, 1):\n",
    "    print(f\"Processing URL {i}/{len(urls)}\")\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        specific_div = soup.find('div', {'class': 'single_tab', 'id': 'recent_tab'})\n",
    "        if specific_div:\n",
    "            new_urls = [a.get('href') for a in specific_div.find_all('a', itemprop='url') if a.get('href') and a.get('href') not in existing_urls]\n",
    "            df = pd.concat([df, pd.DataFrame(new_urls, columns=['Event_URL'])], ignore_index=True)\n",
    "    except requests.RequestException:\n",
    "        print(f\"Failed to process URL {i}\")\n",
    "        pass\n",
    "\n",
    "df.to_csv('./data/event_urls_sherdog.csv', index=False)\n",
    "print(\"Updated event URLs saved.\")\n",
    "\n",
    "# Remove Broken URL's \n",
    "urls_to_delete = {\n",
    "    \"/events/UFC-233-Ultimate-Fighting-Championship-233-72021\",\n",
    "    \"/events/UFC-Fight-Night-97-Lamas-vs-Penn-90890\",\n",
    "    \"/events/UFC-176-Aldo-vs-Mendes-2-37609\",\n",
    "    \"/events/UFC-151-Jones-vs-Henderson-25809\"\n",
    "}\n",
    "df = pd.read_csv(\"./data/event_urls_sherdog.csv\")\n",
    "df = df[~df[\"Event_URL\"].isin(urls_to_delete)]\n",
    "df.to_csv(\"./data/event_urls_sherdog.csv\", index=False)\n",
    "print(\"Broken URLs removed.\")\n",
    "\n",
    "# Event URLs \n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\"\n",
    "]\n",
    "df = pd.read_csv('./data/event_urls_sherdog.csv')\n",
    "df['Event_URL'] = \"https://sherdog.com\" + df['Event_URL'].astype(str)\n",
    "event_dates = []\n",
    "with requests.Session() as session:\n",
    "    session.headers.update({\"User-Agent\": random.choice(user_agents)})\n",
    "    with ThreadPoolExecutor(max_workers=40) as executor:\n",
    "        # futures = {executor.submit(session.get, url, {\"User-Agent\": random.choice(user_agents)}, 30): url for url in df['Event_URL']}\n",
    "        futures = {executor.submit(lambda url: session.get(url, headers={\"User-Agent\": random.choice(user_agents)}, timeout=30), url): url for url in df['Event_URL']}\n",
    "        for future in tqdm(as_completed(futures), total=len(df['Event_URL']), desc=\"Scraping Events\", unit=\"Event\"):\n",
    "            url = futures[future]\n",
    "            try:\n",
    "                response = future.result()\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    event_date_meta = soup.find('meta', itemprop='startDate')\n",
    "                    event_dates.append(event_date_meta['content'].strip() if event_date_meta else None)\n",
    "                else:\n",
    "                    event_dates.append(\"Error\")\n",
    "            except requests.exceptions.RequestException:\n",
    "                event_dates.append(\"Error\")\n",
    "df['Event Date'] = event_dates\n",
    "df.to_csv('./data/event_urls_sherdog.csv', index=False)\n",
    "print(\"Event dates appended successfully.\")\n",
    "print(f\"Total number of rows including the header in event_urls_sherdog.csv: {len(df)}\")\n",
    "print(f\"Column names: {list(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sherdog: Event Data\n",
    "# event_data_sherdog.csv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "}\n",
    "urls_df = pd.read_csv('data/event_urls_sherdog.csv')\n",
    "all_data = []\n",
    "def fetch_event_data(url, session):\n",
    "    full_url = f'https://sherdog.com{url}' if not url.startswith('http') else url\n",
    "    event_data = []\n",
    "    try:\n",
    "        with session.get(full_url, headers=headers) as response:\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                event_name = soup.find('span', itemprop='name').text.strip()\n",
    "                event_location = soup.find('span', itemprop='location').text.strip()\n",
    "                event_date = soup.find('meta', itemprop='startDate')['content'].strip()\n",
    "                main_event_fighters = soup.find_all('div', class_='fighter')\n",
    "                if main_event_fighters:\n",
    "                    fighter1 = main_event_fighters[0].find('span', itemprop='name').text.strip()\n",
    "                    fighter2 = main_event_fighters[1].find('span', itemprop='name').text.strip()\n",
    "                    fighter1_id = main_event_fighters[0].find('a', itemprop='url')['href'].split('-')[-1]\n",
    "                    fighter2_id = main_event_fighters[1].find('a', itemprop='url')['href'].split('-')[-1]\n",
    "                    weight_class = soup.find('span', class_='weight_class').text.strip()\n",
    "                    winning_fighter = fighter1  \n",
    "                    winning_method_em = soup.find('em', string='Method').parent\n",
    "                    winning_method = winning_method_em.contents[2].strip()\n",
    "                    winning_round_em = soup.find('em', string='Round').parent\n",
    "                    winning_round = winning_round_em.contents[2].strip()\n",
    "                    winning_time_em = soup.find('em', string='Time').parent\n",
    "                    winning_time = winning_time_em.contents[2].strip()\n",
    "                    referee_em = soup.find('em', string='Referee').parent\n",
    "                    referee = referee_em.find('a').text.strip()\n",
    "                    event_data.append({\n",
    "                        'Event Name': event_name,\n",
    "                        'Event Location': event_location,\n",
    "                        'Event Date': event_date,\n",
    "                        'Fighter 1': fighter1,\n",
    "                        'Fighter 2': fighter2,\n",
    "                        'Fighter 1 ID': fighter1_id,\n",
    "                        'Fighter 2 ID': fighter2_id,\n",
    "                        'Weight Class': weight_class,\n",
    "                        'Winning Fighter': winning_fighter,\n",
    "                        'Winning Method': winning_method,\n",
    "                        'Winning Round': winning_round,\n",
    "                        'Winning Time': winning_time,\n",
    "                        'Referee': referee,\n",
    "                        'Fight Type': 'Main Event'\n",
    "                    })\n",
    "                other_bouts = soup.find_all('tr', itemprop='subEvent')\n",
    "                for bout in other_bouts:\n",
    "                    fighters = bout.find_all('div', class_='fighter_list')\n",
    "                    if len(fighters) >= 2:\n",
    "                        fighter1 = fighters[0].find('img')['title']\n",
    "                        fighter2 = fighters[1].find('img')['title']\n",
    "                        fighter1_url = fighters[0].find('a', itemprop='url')['href']\n",
    "                        fighter2_url = fighters[1].find('a', itemprop='url')['href']\n",
    "                        fighter1_id = fighter1_url.split('-')[-1]\n",
    "                        fighter2_id = fighter2_url.split('-')[-1]\n",
    "                        weight_class = bout.find('span', class_='weight_class')\n",
    "                        weight_class = weight_class.text.strip() if weight_class else \"Unknown\"\n",
    "                        winning_method = bout.find('td', class_='winby').find('b').get_text(strip=True)\n",
    "                        winning_round = bout.find_all('td')[-2].get_text(strip=True)\n",
    "                        winning_time = bout.find_all('td')[-1].get_text(strip=True)\n",
    "                        referee = bout.find('td', class_='winby').find('a').get_text(strip=True)\n",
    "                        event_data.append({\n",
    "                            'Event Name': event_name,\n",
    "                            'Event Location': event_location,\n",
    "                            'Event Date': event_date,\n",
    "                            'Fighter 1': fighter1,\n",
    "                            'Fighter 2': fighter2,\n",
    "                            'Fighter 1 ID': fighter1_id,\n",
    "                            'Fighter 2 ID': fighter2_id,\n",
    "                            'Weight Class': weight_class,\n",
    "                            'Winning Fighter': fighter1,  \n",
    "                            'Winning Method': winning_method,\n",
    "                            'Winning Round': winning_round,\n",
    "                            'Winning Time': winning_time,\n",
    "                            'Referee': referee,\n",
    "                            'Fight Type': 'Undercard'\n",
    "                        })\n",
    "        return event_data\n",
    "    except Exception as e:\n",
    "        print(f\"Request failed for {full_url}: {e}\")\n",
    "        return None\n",
    "session = requests.Session()\n",
    "total_urls = len(urls_df['Event_URL'])\n",
    "completed_requests = 0\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:\n",
    "    futures = [executor.submit(fetch_event_data, url, session) for url in urls_df['Event_URL']]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        data = future.result()\n",
    "        completed_requests += 1\n",
    "        progress_percentage = (completed_requests / total_urls) * 100\n",
    "        print(f\"Completed {completed_requests}/{total_urls} requests ({progress_percentage:.2f}%)\")\n",
    "        if data:\n",
    "            all_data.extend(data)\n",
    "df = pd.DataFrame(all_data)\n",
    "file_path = './data/event_data_sherdog.csv'\n",
    "write_mode = 'a' if os.path.isfile(file_path) else 'w'\n",
    "df.to_csv(file_path, mode=write_mode, header=not os.path.isfile(file_path), index=False)\n",
    "print(\"Data successfully written to data/event_data_sherdog.csv\")\n",
    "print(f\"Total number of rows: {len(df)}\")\n",
    "print(f\"Column names: {list(df.columns)}\")\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sherdog: Fighter IDs\n",
    "# fighter_id_sherdog.csv\n",
    "\n",
    "df = pd.read_csv('./data/event_data_sherdog.csv')\n",
    "df2 = pd.DataFrame(columns=['Fighter', 'Fighter_ID'])\n",
    "df2.to_csv('./data/fighter_id_sherdog.csv', index=False)\n",
    "for index, row in df.iterrows():\n",
    "    fighter1 = row['Fighter 1']\n",
    "    fighter2 = row['Fighter 2']\n",
    "    fighter1_id = row['Fighter 1 ID']\n",
    "    fighter2_id = row['Fighter 2 ID']\n",
    "    for fighter, fighter_id in zip([fighter1, fighter2], [fighter1_id, fighter2_id]):\n",
    "        if fighter not in df2['Fighter'].values and fighter_id not in df2['Fighter_ID'].values:\n",
    "            df2 = pd.concat([df2, pd.DataFrame([{'Fighter': fighter, 'Fighter_ID': fighter_id}])])  # adjusted line\n",
    "df2.to_csv('./data/fighter_id_sherdog.csv', index=False)\n",
    "print(\"Data successfully written to data/fighter_id_sherdog.csv\")\n",
    "print(f\"Total number of rows: {len(df2)}\")\n",
    "print(f\"Column names: {list(df2.columns)}\")\n",
    "\n",
    "# Remove nicknames\n",
    "df = pd.read_csv('./data/fighter_id_sherdog.csv')\n",
    "df['Fighter'] = df['Fighter'].str.replace(r\" '.+?'\", \"\", regex=True)\n",
    "df.to_csv('./data/fighter_id_sherdog.csv', index=False)\n",
    "df = pd.read_csv('./data/event_data_sherdog.csv')\n",
    "for col in ['Fighter 1', 'Fighter 2', 'Winning Fighter']:\n",
    "    df[col] = df[col].str.replace(r\" '.+?'\", \"\", regex=True)\n",
    "df.to_csv('./data/event_data_sherdog.csv', index=False)\n",
    "\n",
    "# Add 'UFC' indicator to current fighters in fighter_id_sherdog.csv.csv\n",
    "df = pd.read_csv('./data/fighter_id_sherdog.csv')\n",
    "df['UFC'] = 'y'\n",
    "df.to_csv('./data/fighter_id_sherdog.csv', index=False)\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "df.to_csv('./data/fighter_id_sherdog.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sherdog: Fighter Info\n",
    "# fighter_info.csv\n",
    "\n",
    "def scrape_fighter_general_info_sherdog(fighter, fighter_id):\n",
    "    url = f'https://www.sherdog.com/fighter/{fighter_id}'\n",
    "    headers = {\"User-Agent\": \"'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\"}\n",
    "    \n",
    "    # Add retry logic\n",
    "    max_retries = 6  # Try for up to 1 minute (6 * 10 seconds)\n",
    "    retry_count = 0\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                break\n",
    "        except (requests.exceptions.ConnectionError, requests.exceptions.RequestException) as e:\n",
    "            retry_count += 1\n",
    "            if retry_count == max_retries:\n",
    "                print(f\"Failed to fetch data for fighter {fighter} after {max_retries} retries\")\n",
    "                return {}\n",
    "            print(f\"Connection error, retrying in 10 seconds... (Attempt {retry_count}/{max_retries})\")\n",
    "            time.sleep(10)\n",
    "            continue\n",
    "            \n",
    "    if response.status_code != 200:\n",
    "        return {}\n",
    "        \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    fighter_dict = {}\n",
    "    try:\n",
    "        fighter_data = soup.find('div', class_='fighter-data')\n",
    "    except AttributeError:\n",
    "        fighter_data = None\n",
    "    try:\n",
    "        birthdate = soup.find('span', itemprop='birthDate')\n",
    "        birthdate = (birthdate.text).strip('\"\"')\n",
    "    except AttributeError:\n",
    "        birthdate = '-'\n",
    "    try:\n",
    "        nationality = soup.find('strong', itemprop='nationality')\n",
    "        nationality = (nationality.text).strip()\n",
    "    except AttributeError:\n",
    "        nationality = '-'\n",
    "    try:\n",
    "        hometown = soup.find('span', {'itemprop': 'addressLocality'}).text\n",
    "        hometown = hometown.strip()\n",
    "    except AttributeError:\n",
    "        hometown = '-'\n",
    "    try:\n",
    "        association = soup.find('span', {'itemprop': 'name'}).text\n",
    "        association = association.strip()\n",
    "    except AttributeError:\n",
    "        association = '-'\n",
    "    try:\n",
    "        weight_class_div = fighter_data.find('div', {'class': 'association-class'})\n",
    "        links = weight_class_div.find_all('a')\n",
    "        weight_class = links[-1].text\n",
    "        weight_class = weight_class.strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        weight_class = ''\n",
    "    try:\n",
    "        nickname = soup.find('span', class_='nickname')\n",
    "        nickname = (nickname.text).strip('\"')\n",
    "    except AttributeError:\n",
    "        nickname = '-'\n",
    "    try:\n",
    "        height = soup.find('b', itemprop='height')\n",
    "        height = (height.text).strip('\"')\n",
    "    except AttributeError:\n",
    "        height = '-'\n",
    "    try:\n",
    "        wins = soup.find('div', class_='winloses win').find_all('span')[1]\n",
    "        wins = (wins.text).strip()\n",
    "    except AttributeError:\n",
    "        wins = '-'\n",
    "    try:\n",
    "        losses = soup.find('div', class_='winloses lose').find_all('span')[1]\n",
    "        losses = (losses.text).strip()\n",
    "    except AttributeError:\n",
    "        losses = '-'\n",
    "    dec_data_list = []\n",
    "    try:\n",
    "        win_type = fighter_data.find_all('div', class_='meter-title', string='DECISIONS')\n",
    "        for method in win_type:\n",
    "            if method.text.startswith('DECISIONS'):\n",
    "                dec_data = method.find_next('div', class_='pl').text\n",
    "                dec_data_list.append(dec_data)\n",
    "        wins_dec = (dec_data_list[0]).strip()\n",
    "        losses_dec = (dec_data_list[1]).strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        wins_dec = '-'\n",
    "        losses_dec = '-'\n",
    "    ko_data_list = []\n",
    "    try:\n",
    "        win_type = soup.find_all('div', class_='meter-title')\n",
    "        for method in win_type:\n",
    "            if method.text.startswith('KO'):\n",
    "                ko_data = method.find_next('div', class_='pl').text\n",
    "                ko_data_list.append(ko_data)\n",
    "        wins_ko = (ko_data_list[0]).strip()\n",
    "        losses_ko = (ko_data_list[1]).strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        wins_ko = '-'\n",
    "        losses_ko = '-'\n",
    "    sub_data_list = []\n",
    "    try:\n",
    "        win_type = fighter_data.find_all('div', class_='meter-title', string='SUBMISSIONS')\n",
    "        for method in win_type:\n",
    "            if method.text.startswith('SUBMISSIONS'):\n",
    "                sub_data = method.find_next('div', class_='pl').text\n",
    "                sub_data_list.append(sub_data)\n",
    "        wins_sub = (sub_data_list[0]).strip()\n",
    "        losses_sub = (sub_data_list[1]).strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        wins_sub = '-'\n",
    "        losses_sub = '-'\n",
    "    fighter_dict = {\n",
    "        'Fighter': fighter,\n",
    "        'Nickname': nickname,\n",
    "        'Birth Date': birthdate,\n",
    "        'Nationality': nationality,\n",
    "        'Hometown': hometown,\n",
    "        'Association': association,\n",
    "        'Weight Class': weight_class,\n",
    "        'Height': height,\n",
    "        'Wins': wins,\n",
    "        'Losses': losses,\n",
    "        'Win_Decision': wins_dec,\n",
    "        'Win_KO': wins_ko,\n",
    "        'Win_Sub': wins_sub,\n",
    "        'Loss_Decision': losses_dec,\n",
    "        'Loss_KO': losses_ko,\n",
    "        'Loss_Sub': losses_sub,\n",
    "        'Fighter_ID': fighter_id\n",
    "    }\n",
    "    return fighter_dict\n",
    "\n",
    "def scrape_fighters_concurrently():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    df_fighter_id = pd.read_csv('./data/fighter_id_sherdog.csv')\n",
    "    fighter_data_list = []\n",
    "    total_fighters = len(df_fighter_id)\n",
    "    fighters_processed = 0\n",
    "    with ThreadPoolExecutor(max_workers=40) as executor:\n",
    "        future_to_fighter = {executor.submit(scrape_fighter_general_info_sherdog, row['Fighter'], row['Fighter_ID']): row for index, row in df_fighter_id.iterrows()}\n",
    "        for future in tqdm(as_completed(future_to_fighter), total=len(future_to_fighter)):\n",
    "            fighter_data = future.result()\n",
    "            if fighter_data:\n",
    "                fighter_data_list.append(fighter_data)\n",
    "            fighters_processed += 1\n",
    "            print(f\"Scraping Fighter Info: {fighters_processed}/{total_fighters} fighters processed\")\n",
    "    new_df = pd.DataFrame(fighter_data_list)\n",
    "    new_df.to_csv('./data/fighter_info.csv', index=False)\n",
    "scrape_fighters_concurrently()\n",
    "\n",
    "df = pd.read_csv('./data/fighter_info.csv')\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "df.to_csv('./data/fighter_info.csv', index=False)\n",
    "print(\"Data successfully written to data/fighter_info.csv\")\n",
    "print(f\"Total number of rows: {len(df)}\")\n",
    "print(f\"Column names: {list(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sherdog: Fighters\n",
    "# data/fighters/*\n",
    "\n",
    "# os.makedirs('./data/fighters/', exist_ok=True)\n",
    "# def scrape_fighter_fights_sherdog(fighter_name, fighter_id, fighter_url):\n",
    "#     headers = {\n",
    "#         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "#     }\n",
    "#     response = requests.get(fighter_url, headers=headers, timeout=30)\n",
    "#     if response.status_code == 200:\n",
    "#         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#         table = soup.find('table', {'class': 'new_table fighter'})\n",
    "#         rows = table.find_all('tr')[1:]\n",
    "#         fight_data = []\n",
    "#         new_opponents = []\n",
    "#         for row in rows:\n",
    "#             cols = row.find_all('td')\n",
    "#             fight_dict = {\n",
    "#                 'Result': cols[0].text.strip(),\n",
    "#                 'Opponent': cols[1].find('a').text.strip() if cols[1].find('a') else '-',\n",
    "#                 'Event Date': cols[2].find_all('span')[-1].text.strip() if cols[2].find_all('span') else '-',\n",
    "#                 'Method/Referee': cols[3].text.strip().split('\\n')[0],\n",
    "#                 'Rounds': cols[4].text.strip(),\n",
    "#                 'Time': cols[5].text.strip()\n",
    "#             }\n",
    "#             fight_data.append(fight_dict)\n",
    "#             opponent_link = cols[1].find('a')['href'] if cols[1].find('a') else None\n",
    "#             if opponent_link:\n",
    "#                 opponent_id = opponent_link.split('-')[-1]\n",
    "#                 new_opponents.append({'Fighter': fight_dict['Opponent'], 'Fighter_ID': opponent_id})\n",
    "#         return fighter_id, fighter_name, fight_data, new_opponents\n",
    "#     print(f\"Failed to retrieve page for {fighter_name} (Status code: {response.status_code})\")\n",
    "#     return fighter_id, fighter_name, [], []\n",
    "\n",
    "# df_fighter_id = pd.read_csv('./data/fighter_id_sherdog.csv')\n",
    "# all_new_opponents = []\n",
    "\n",
    "# def process_fighter(row):\n",
    "#     fighter_name = row['Fighter']\n",
    "#     fighter_id = row['Fighter_ID']\n",
    "#     fighter_file = f\"./data/fighters/{fighter_name.replace(' ', '_')}_{fighter_id}.csv\"\n",
    "    \n",
    "#     # Skip if file already exists\n",
    "#     if os.path.exists(fighter_file):\n",
    "#         print(f\"Skipping {fighter_name} - file already exists\")\n",
    "#         return fighter_id, fighter_name, [], []\n",
    "        \n",
    "#     fighter_url = f\"https://www.sherdog.com/fighter/{fighter_name.replace(' ', '-')}-{fighter_id}\"\n",
    "#     return scrape_fighter_fights_sherdog(fighter_name, fighter_id, fighter_url)\n",
    "\n",
    "# total_fighters = len(df_fighter_id)\n",
    "# fighters_processed = 0\n",
    "# print(f\"\\nStarting to process {total_fighters} fighters\")\n",
    "\n",
    "# with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "#     futures = [executor.submit(process_fighter, row) for _, row in df_fighter_id.iterrows()]\n",
    "#     for future in as_completed(futures):\n",
    "#         fighter_id, fighter_name, fight_data, new_opponents = future.result()\n",
    "#         fighters_processed += 1\n",
    "#         print(f\"Progress: {fighters_processed}/{total_fighters} fighters processed ({(fighters_processed/total_fighters)*100:.1f}%)\")\n",
    "#         if fight_data:\n",
    "#             output_file = f\"./data/fighters/{fighter_name.replace(' ', '_')}_{fighter_id}.csv\"\n",
    "#             pd.DataFrame(fight_data).to_csv(output_file, index=False)\n",
    "#             print(f\"Saved {len(fight_data)} fights for {fighter_name} to {output_file}\")\n",
    "#             all_new_opponents.extend(new_opponents)\n",
    "\n",
    "# if all_new_opponents:\n",
    "#     print(f\"\\nFound {len(all_new_opponents)} new opponents\")\n",
    "#     df_new_opponents = pd.DataFrame(all_new_opponents).drop_duplicates()\n",
    "#     df_fighter_id = pd.concat([df_fighter_id, df_new_opponents], ignore_index=True).drop_duplicates()\n",
    "#     df_fighter_id.to_csv('./data/fighter_id_sherdog.csv', index=False)\n",
    "#     print(f\"Updated fighter_id_sherdog.csv with {len(df_new_opponents)} new unique opponents\")\n",
    "\n",
    "\n",
    "\n",
    "# Sherdog: Fighters\n",
    "# data/fighters/*\n",
    "os.makedirs('./data/fighters/', exist_ok=True)\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "})\n",
    "def scrape_fighter_fights_sherdog(session, fighter_name, fighter_id, fighter_url):\n",
    "    try:\n",
    "        response = session.get(fighter_url, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            table = soup.find('table', {'class': 'new_table fighter'})\n",
    "            rows = table.find_all('tr')[1:] if table else []\n",
    "            fight_data = []\n",
    "            new_opponents = []\n",
    "            for row in rows:\n",
    "                cols = row.find_all('td')\n",
    "                fight_dict = {\n",
    "                    'Result': cols[0].text.strip(),\n",
    "                    'Opponent': cols[1].find('a').text.strip() if cols[1].find('a') else '-',\n",
    "                    'Event Date': cols[2].find_all('span')[-1].text.strip() if cols[2].find_all('span') else '-',\n",
    "                    'Method/Referee': cols[3].text.strip().split('\\n')[0],\n",
    "                    'Rounds': cols[4].text.strip(),\n",
    "                    'Time': cols[5].text.strip()\n",
    "                }\n",
    "                fight_data.append(fight_dict)\n",
    "                opponent_link = cols[1].find('a')['href'] if cols[1].find('a') else None\n",
    "                if opponent_link:\n",
    "                    opponent_id = opponent_link.split('-')[-1]\n",
    "                    new_opponents.append({'Fighter': fight_dict['Opponent'], 'Fighter_ID': opponent_id})\n",
    "            return fighter_id, fighter_name, fight_data, new_opponents\n",
    "        print(f\"Failed to retrieve page for {fighter_name} (Status code: {response.status_code})\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error for {fighter_name}: {e}\")\n",
    "    return fighter_id, fighter_name, [], []\n",
    "df_fighter_id = pd.read_csv('./data/fighter_id_sherdog.csv')\n",
    "all_new_opponents = []\n",
    "def process_fighter(row, session):\n",
    "    fighter_name = row['Fighter']\n",
    "    fighter_id = row['Fighter_ID']\n",
    "    fighter_file = f\"./data/fighters/{fighter_name.replace(' ', '_')}_{fighter_id}.csv\"\n",
    "    if os.path.exists(fighter_file):\n",
    "        print(f\"Skipping {fighter_name} - file already exists\")\n",
    "        return fighter_id, fighter_name, [], []\n",
    "        \n",
    "    fighter_url = f\"https://www.sherdog.com/fighter/{fighter_name.replace(' ', '-')}-{fighter_id}\"\n",
    "    return scrape_fighter_fights_sherdog(session, fighter_name, fighter_id, fighter_url)\n",
    "\n",
    "total_fighters = len(df_fighter_id)\n",
    "fighters_processed = 0\n",
    "print(f\"\\nStarting to process {total_fighters} fighters\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    futures = [executor.submit(process_fighter, row, session) for _, row in df_fighter_id.iterrows()]\n",
    "    for future in as_completed(futures):\n",
    "        fighter_id, fighter_name, fight_data, new_opponents = future.result()\n",
    "        fighters_processed += 1\n",
    "        print(f\"Progress: {fighters_processed}/{total_fighters} fighters processed ({(fighters_processed/total_fighters)*100:.1f}%)\")\n",
    "        \n",
    "        if fight_data:\n",
    "            output_file = f\"./data/fighters/{fighter_name.replace(' ', '_')}_{fighter_id}.csv\"\n",
    "            pd.DataFrame(fight_data).to_csv(output_file, index=False)\n",
    "            print(f\"Saved {len(fight_data)} fights for {fighter_name} to {output_file}\")\n",
    "            all_new_opponents.extend(new_opponents)\n",
    "\n",
    "if all_new_opponents:\n",
    "    print(f\"\\nFound {len(all_new_opponents)} new opponents\")\n",
    "    df_new_opponents = pd.DataFrame(all_new_opponents).drop_duplicates()\n",
    "    df_fighter_id = pd.concat([df_fighter_id, df_new_opponents], ignore_index=True).drop_duplicates()\n",
    "    df_fighter_id.to_csv('./data/fighter_id_sherdog.csv', index=False)\n",
    "    print(f\"Updated fighter_id_sherdog.csv with {len(df_new_opponents)} new unique opponents\")\n",
    "\n",
    "\n",
    "# ### WITH RETRIES ###\n",
    "# # Sherdog: Fighters\n",
    "# # data/fighters/*\n",
    "# os.makedirs('./data/fighters/', exist_ok=True)\n",
    "# session = requests.Session()\n",
    "# session.headers.update({\n",
    "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "# })\n",
    "\n",
    "# def scrape_fighter_fights_sherdog(session, fighter_name, fighter_id, fighter_url):\n",
    "#     max_retries = 4\n",
    "#     for attempt in range(1, max_retries + 1):\n",
    "#         try:\n",
    "#             response = session.get(fighter_url, timeout=30)\n",
    "#             if response.status_code == 200:\n",
    "#                 soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#                 table = soup.find('table', {'class': 'new_table fighter'})\n",
    "#                 rows = table.find_all('tr')[1:] if table else []\n",
    "#                 fight_data = []\n",
    "#                 new_opponents = []\n",
    "#                 for row in rows:\n",
    "#                     cols = row.find_all('td')\n",
    "#                     fight_dict = {\n",
    "#                         'Result': cols[0].text.strip(),\n",
    "#                         'Opponent': cols[1].find('a').text.strip() if cols[1].find('a') else '-',\n",
    "#                         'Event Date': cols[2].find_all('span')[-1].text.strip() if cols[2].find_all('span') else '-',\n",
    "#                         'Method/Referee': cols[3].text.strip().split('\\n')[0],\n",
    "#                         'Rounds': cols[4].text.strip(),\n",
    "#                         'Time': cols[5].text.strip()\n",
    "#                     }\n",
    "#                     fight_data.append(fight_dict)\n",
    "#                     opponent_link = cols[1].find('a')['href'] if cols[1].find('a') else None\n",
    "#                     if opponent_link:\n",
    "#                         opponent_id = opponent_link.split('-')[-1]\n",
    "#                         new_opponents.append({'Fighter': fight_dict['Opponent'], 'Fighter_ID': opponent_id})\n",
    "#                 return fighter_id, fighter_name, fight_data, new_opponents\n",
    "#             else:\n",
    "#                 print(f\"Failed to retrieve page for {fighter_name} (Status code: {response.status_code}). Attempt {attempt}/{max_retries}\")\n",
    "#         except requests.exceptions.RequestException as e:\n",
    "#             if attempt < max_retries:\n",
    "#                 print(f\"Request error for {fighter_name}: {e}. Retrying (attempt {attempt}/{max_retries})...\")\n",
    "#             else:\n",
    "#                 print(f\"Request error for {fighter_name}: {e}. No more retries left.\")\n",
    "#     return fighter_id, fighter_name, [], []\n",
    "# df_fighter_id = pd.read_csv('./data/fighter_id_sherdog.csv')\n",
    "# all_new_opponents = []\n",
    "# def process_fighter(row, session):\n",
    "#     fighter_name = row['Fighter']\n",
    "#     fighter_id = row['Fighter_ID']\n",
    "#     fighter_file = f\"./data/fighters/{fighter_name.replace(' ', '_')}_{fighter_id}.csv\"\n",
    "#     if os.path.exists(fighter_file):\n",
    "#         print(f\"Skipping {fighter_name} - file already exists\")\n",
    "#         return fighter_id, fighter_name, [], []\n",
    "        \n",
    "#     fighter_url = f\"https://www.sherdog.com/fighter/{fighter_name.replace(' ', '-')}-{fighter_id}\"\n",
    "#     return scrape_fighter_fights_sherdog(session, fighter_name, fighter_id, fighter_url)\n",
    "\n",
    "# total_fighters = len(df_fighter_id)\n",
    "# fighters_processed = 0\n",
    "# print(f\"\\nStarting to process {total_fighters} fighters\")\n",
    "\n",
    "# with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "# # with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "#     futures = [executor.submit(process_fighter, row, session) for _, row in df_fighter_id.iterrows()]\n",
    "#     for future in as_completed(futures):\n",
    "#         fighter_id, fighter_name, fight_data, new_opponents = future.result()\n",
    "#         fighters_processed += 1\n",
    "#         print(f\"Progress: {fighters_processed}/{total_fighters} fighters processed ({(fighters_processed/total_fighters)*100:.1f}%)\")\n",
    "        \n",
    "#         if fight_data:\n",
    "#             output_file = f\"./data/fighters/{fighter_name.replace(' ', '_')}_{fighter_id}.csv\"\n",
    "#             pd.DataFrame(fight_data).to_csv(output_file, index=False)\n",
    "#             print(f\"Saved {len(fight_data)} fights for {fighter_name} to {output_file}\")\n",
    "#             all_new_opponents.extend(new_opponents)\n",
    "\n",
    "# if all_new_opponents:\n",
    "#     print(f\"\\nFound {len(all_new_opponents)} new opponents\")\n",
    "#     df_new_opponents = pd.DataFrame(all_new_opponents).drop_duplicates()\n",
    "#     df_fighter_id = pd.concat([df_fighter_id, df_new_opponents], ignore_index=True).drop_duplicates()\n",
    "#     df_fighter_id.to_csv('./data/fighter_id_sherdog.csv', index=False)\n",
    "#     print(f\"Updated fighter_id_sherdog.csv with {len(df_new_opponents)} new unique opponents\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sherdog: Fighters\n",
    "# data/fighters/*\n",
    "# os.makedirs('./data/fighters/', exist_ok=True)\n",
    "# def scrape_fighter_fights_sherdog(fighter_name, fighter_id, fighter_url):\n",
    "#     try:\n",
    "#         headers = {\n",
    "#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "#         }\n",
    "#         response = requests.get(fighter_url, headers=headers, timeout=30)\n",
    "#         response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "        \n",
    "#         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#         table = soup.find('table', {'class': 'new_table fighter'})\n",
    "        \n",
    "#         if not table:\n",
    "#             print(f\"Warning: No fight table found for {fighter_name} (ID: {fighter_id}) at {fighter_url}\")\n",
    "#             return fighter_id, fighter_name, [], []\n",
    "            \n",
    "#         rows = table.find_all('tr')[1:]\n",
    "#         fight_data = []\n",
    "#         new_opponents = []\n",
    "        \n",
    "#         for row in rows:\n",
    "#             try:\n",
    "#                 cols = row.find_all('td')\n",
    "#                 if len(cols) < 6:  # Check if row has expected number of columns\n",
    "#                     continue\n",
    "                    \n",
    "#                 opponent_elem = cols[1].find('a')\n",
    "#                 opponent_name = opponent_elem.text.strip() if opponent_elem else '-'\n",
    "                \n",
    "#                 fight_dict = {\n",
    "#                     'Result': cols[0].text.strip(),\n",
    "#                     'Opponent': opponent_name,\n",
    "#                     'Event Date': cols[2].find_all('span')[-1].text.strip() if cols[2].find_all('span') else '-',\n",
    "#                     'Method/Referee': cols[3].text.strip().split('\\n')[0],\n",
    "#                     'Rounds': cols[4].text.strip(),\n",
    "#                     'Time': cols[5].text.strip()\n",
    "#                 }\n",
    "#                 fight_data.append(fight_dict)\n",
    "                \n",
    "#                 if opponent_elem and opponent_elem.get('href'):\n",
    "#                     opponent_id = opponent_elem['href'].split('-')[-1]\n",
    "#                     new_opponents.append({'Fighter': opponent_name, 'Fighter_ID': opponent_id})\n",
    "                    \n",
    "#             except Exception as e:\n",
    "#                 print(f\"Warning: Error processing fight row for {fighter_name}: {str(e)}\")\n",
    "#                 continue\n",
    "                \n",
    "#         return fighter_id, fighter_name, fight_data, new_opponents\n",
    "        \n",
    "#     except requests.Timeout:\n",
    "#         print(f\"Error: Timeout while scraping {fighter_name} (ID: {fighter_id}) at {fighter_url}\")\n",
    "#         return fighter_id, fighter_name, [], []\n",
    "#     except requests.RequestException as e:\n",
    "#         print(f\"Error: Network error while scraping {fighter_name} (ID: {fighter_id}) at {fighter_url}: {str(e)}\")\n",
    "#         return fighter_id, fighter_name, [], []\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: Unexpected error while scraping {fighter_name} (ID: {fighter_id}) at {fighter_url}: {str(e)}\")\n",
    "#         return fighter_id, fighter_name, [], []\n",
    "\n",
    "# df_fighter_id = pd.read_csv('./data/fighter_id_sherdog.csv')\n",
    "# all_new_opponents = []\n",
    "\n",
    "# def process_fighter(row):\n",
    "#     try:\n",
    "#         fighter_url = f\"https://www.sherdog.com/fighter/{row['Fighter'].replace(' ', '-')}-{row['Fighter_ID']}\"\n",
    "#         return scrape_fighter_fights_sherdog(row['Fighter'], row['Fighter_ID'], fighter_url)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing fighter {row['Fighter']}: {str(e)}\")\n",
    "#         return row['Fighter_ID'], row['Fighter'], [], []\n",
    "\n",
    "# total_fighters = len(df_fighter_id)\n",
    "# fighters_processed = 0\n",
    "\n",
    "# with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "#     futures = [executor.submit(process_fighter, row) for _, row in df_fighter_id.iterrows()]\n",
    "#     for future in as_completed(futures):\n",
    "#         try:\n",
    "#             fighter_id, fighter_name, fight_data, new_opponents = future.result()\n",
    "#             fighters_processed += 1\n",
    "#             print(f\"Processed {fighters_processed}/{total_fighters} fighters: {fighter_name}\")\n",
    "            \n",
    "#             if fight_data:\n",
    "#                 output_file = f\"./data/fighters/{fighter_name.replace(' ', '_')}_{fighter_id}.csv\"\n",
    "#                 pd.DataFrame(fight_data).to_csv(output_file, index=False)\n",
    "#                 all_new_opponents.extend(new_opponents)\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing future result: {str(e)}\")\n",
    "#             continue\n",
    "\n",
    "# if all_new_opponents:\n",
    "#     try:\n",
    "#         df_new_opponents = pd.DataFrame(all_new_opponents).drop_duplicates()\n",
    "#         df_fighter_id = pd.concat([df_fighter_id, df_new_opponents], ignore_index=True).drop_duplicates()\n",
    "#         df_fighter_id.to_csv('./data/fighter_id_sherdog.csv', index=False)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error saving new opponents data: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # os.makedirs('./data/fighters/')\n",
    "# # from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# # def scrape_fighter_fights_sherdog(fighter_name, fighter_id, fighter_url):\n",
    "# #     headers = {\n",
    "# #         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "# #     }\n",
    "# #     response = requests.get(fighter_url, headers=headers, timeout=(10, 30))\n",
    "# #     if response.status_code == 200:\n",
    "# #         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "# #         table = soup.find('table', {'class': 'new_table fighter'})\n",
    "# #         rows = table.find_all('tr')[1:]\n",
    "# #         fight_data = []\n",
    "# #         new_opponents = []\n",
    "# #         for row in rows:\n",
    "# #             cols = row.find_all('td')\n",
    "# #             fight_dict = {\n",
    "# #                 'Result': cols[0].text.strip(),\n",
    "# #                 'Opponent': cols[1].find('a').text.strip() if cols[1].find('a') else '-',\n",
    "# #                 'Event Date': cols[2].find_all('span')[-1].text.strip() if cols[2].find_all('span') else '-',\n",
    "# #                 'Method/Referee': cols[3].text.strip().split('\\n')[0],\n",
    "# #                 'Rounds': cols[4].text.strip(),\n",
    "# #                 'Time': cols[5].text.strip()\n",
    "# #             }\n",
    "# #             fight_data.append(fight_dict)\n",
    "# #             opponent_link = cols[1].find('a')['href'] if cols[1].find('a') else None\n",
    "# #             if opponent_link:\n",
    "# #                 opponent_id = opponent_link.split('-')[-1]\n",
    "# #                 new_opponents.append({'Fighter': fight_dict['Opponent'], 'Fighter_ID': opponent_id})\n",
    "# #         return fighter_id, fighter_name, fight_data, new_opponents\n",
    "# #     return fighter_id, fighter_name, [], []\n",
    "# # df_fighter_id = pd.read_csv('./data/fighter_id_sherdog.csv')\n",
    "# # all_new_opponents = []\n",
    "# # def process_fighter(row):\n",
    "# #     fighter_url = f\"https://www.sherdog.com/fighter/{row['Fighter'].replace(' ', '-')}-{row['Fighter_ID']}\"\n",
    "# #     return scrape_fighter_fights_sherdog(row['Fighter'], row['Fighter_ID'], fighter_url)\n",
    "# # total_fighters = len(df_fighter_id)\n",
    "# # fighters_processed = 0\n",
    "# # with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "# #     futures = [executor.submit(process_fighter, row) for _, row in df_fighter_id.iterrows()]\n",
    "# #     for future in as_completed(futures):\n",
    "# #     # for future in tqdm(as_completed(futures), total=len(futures), desc=\"Scraping Fighter Data\", unit=\"fighter\"):\n",
    "# #         time.sleep(0.1)  # Adding a small sleep between request submissions\n",
    "# #         fighter_id, fighter_name, fight_data, new_opponents = future.result()\n",
    "# #         fighters_processed += 1\n",
    "# #         print(f\"Processed {fighters_processed}/{total_fighters} fighters: {fighter_name}\")\n",
    "# #         if fight_data:\n",
    "# #             pd.DataFrame(fight_data).to_csv(f\"./data/fighters/{fighter_name.replace(' ', '_')}_{fighter_id}.csv\", index=False)\n",
    "# #             all_new_opponents.extend(new_opponents)\n",
    "# # if all_new_opponents:\n",
    "# #     df_new_opponents = pd.DataFrame(all_new_opponents).drop_duplicates()\n",
    "# #     df_fighter_id = pd.concat([df_fighter_id, df_new_opponents], ignore_index=True).drop_duplicates()\n",
    "# #     df_fighter_id.to_csv('./data/fighter_id_sherdog.csv', index=False)\n",
    "\n",
    "# # directory_path = './data/fighters/'\n",
    "# # for filename in os.listdir(directory_path):\n",
    "# #     if filename.endswith('.csv'):  # Check if the file is a CSV\n",
    "# #         file_path = os.path.join(directory_path, filename)\n",
    "# #         df = pd.read_csv(file_path)\n",
    "# #         df = df.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "# #         df.to_csv(file_path, index=False)\n",
    "\n",
    "# # If you want a progress bar, uncomment these lines:\n",
    "# # from tqdm.notebook import tqdm\n",
    "\n",
    "# user_agents = [\n",
    "#     \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "#     \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "#     \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0 Safari/537.36\",\n",
    "#     \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0\"\n",
    "# ]\n",
    "\n",
    "# os.makedirs('./data/fighters/', exist_ok=True)\n",
    "\n",
    "# def scrape_fighter_fights_sherdog(fighter_name, fighter_id, fighter_url, max_attempts=3):\n",
    "#     # Retries up to max_attempts times if there's a RequestException\n",
    "#     for attempt in range(max_attempts):\n",
    "#         headers = {\n",
    "#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "#         }\n",
    "#         try:\n",
    "#             # Separate connect/read timeouts, e.g., 10s to connect, 30s to read\n",
    "#             response = requests.get(fighter_url, headers=headers, timeout=(10, 30))\n",
    "#             if response.status_code == 200:\n",
    "#                 soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#                 table = soup.find('table', {'class': 'new_table fighter'})\n",
    "#                 if not table:\n",
    "#                     return fighter_id, fighter_name, [], []\n",
    "\n",
    "#                 rows = table.find_all('tr')[1:]\n",
    "#                 fight_data = []\n",
    "#                 new_opponents = []\n",
    "\n",
    "#                 for row in rows:\n",
    "#                     cols = row.find_all('td')\n",
    "#                     fight_dict = {\n",
    "#                         'Result': cols[0].text.strip(),\n",
    "#                         'Opponent': cols[1].find('a').text.strip() if cols[1].find('a') else '-',\n",
    "#                         'Event Date': cols[2].find_all('span')[-1].text.strip() if cols[2].find_all('span') else '-',\n",
    "#                         'Method/Referee': cols[3].text.strip().split('\\n')[0],\n",
    "#                         'Rounds': cols[4].text.strip(),\n",
    "#                         'Time': cols[5].text.strip()\n",
    "#                     }\n",
    "#                     fight_data.append(fight_dict)\n",
    "\n",
    "#                     opponent_link = cols[1].find('a')['href'] if cols[1].find('a') else None\n",
    "#                     if opponent_link:\n",
    "#                         opponent_id = opponent_link.split('-')[-1]\n",
    "#                         new_opponents.append({'Fighter': fight_dict['Opponent'], 'Fighter_ID': opponent_id})\n",
    "\n",
    "#                 return fighter_id, fighter_name, fight_data, new_opponents\n",
    "#             # If it's not a 200, just break to handle below, or keep looping?\n",
    "#             # time.sleep(3)  # small backoff before next attempt\n",
    "#         except requests.RequestException:\n",
    "#             # If there's a network error, wait and retry\n",
    "#             if attempt < max_attempts - 1:\n",
    "#                 time.sleep(5)\n",
    "#     # If all attempts fail or status never 200, return empty\n",
    "#     return fighter_id, fighter_name, [], []\n",
    "\n",
    "# # Load known fighter IDs\n",
    "# fighter_id_path = './data/fighter_id_sherdog.csv'\n",
    "# df_fighter_id = pd.read_csv(fighter_id_path)\n",
    "\n",
    "# all_new_opponents = []\n",
    "\n",
    "# def process_fighter(row):\n",
    "#     fighter_url = f\"https://www.sherdog.com/fighter/{row['Fighter'].replace(' ', '-')}-{row['Fighter_ID']}\"\n",
    "#     return scrape_fighter_fights_sherdog(row['Fighter'], row['Fighter_ID'], fighter_url)\n",
    "\n",
    "# total_fighters = len(df_fighter_id)\n",
    "# fighters_processed = 0\n",
    "\n",
    "# with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "#     futures = [executor.submit(process_fighter, row) for _, row in df_fighter_id.iterrows()]\n",
    "\n",
    "#     # If you want a progress bar, uncomment the tqdm usage:\n",
    "#     # for future in tqdm(as_completed(futures), total=len(futures), desc=\"Scraping Fighter Data\", unit=\"fighter\"):\n",
    "#     for future in as_completed(futures):\n",
    "#         # Add a small sleep (100ms) between processed fighters to avoid flooding\n",
    "#         time.sleep(0.5)\n",
    "#         fighter_id, fighter_name, fight_data, new_opponents = future.result()\n",
    "#         fighters_processed += 1\n",
    "#         print(f\"Processed {fighters_processed}/{total_fighters} fighters: {fighter_name}\")\n",
    "\n",
    "#         if fight_data:\n",
    "#             file_name = f\"{fighter_name.replace(' ', '_')}_{fighter_id}.csv\"\n",
    "#             pd.DataFrame(fight_data).to_csv(f\"./data/fighters/{file_name}\", index=False)\n",
    "#             all_new_opponents.extend(new_opponents)\n",
    "\n",
    "# # Append new opponents to fighter_id_sherdog.csv\n",
    "# if all_new_opponents:\n",
    "#     df_new_opponents = pd.DataFrame(all_new_opponents).drop_duplicates()\n",
    "#     df_fighter_id = pd.concat([df_fighter_id, df_new_opponents], ignore_index=True).drop_duplicates()\n",
    "#     df_fighter_id.to_csv(fighter_id_path, index=False)\n",
    "\n",
    "# # Clean up each CSV in data/fighters/\n",
    "# directory_path = './data/fighters/'\n",
    "# for filename in os.listdir(directory_path):\n",
    "#     if filename.endswith('.csv'):\n",
    "#         file_path = os.path.join(directory_path, filename)\n",
    "#         df = pd.read_csv(file_path)\n",
    "#         df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "#         df.to_csv(file_path, index=False)\n",
    "\n",
    "# print(\"Finished scraping fighter fights.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "first_name = \"Dustin\"\n",
    "last_name = \"Poirier\"\n",
    "file_pattern = f'./data/fighters/*{first_name}*{last_name}*.csv'\n",
    "matching_files = glob.glob(file_pattern)\n",
    "if matching_files:\n",
    "    first_file = matching_files[0]\n",
    "    df = pd.read_csv(first_file)\n",
    "    print(f\"Total number of rows including the header in {first_file}: {len(df)}\")\n",
    "    print(f\"Column names: {list(df.columns)}\")\n",
    "else:\n",
    "    print(\"No files found matching the pattern.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sherdog: Cleaning\n",
    "\n",
    "fighter_info_df = pd.read_csv('./data/fighter_info.csv')\n",
    "event_data_df = pd.read_csv('./data/event_data_sherdog.csv')\n",
    "print(f\"Number of rows in fighter_info.csv: {len(fighter_info_df)}\")\n",
    "print(f\"Number of rows in event_data_sherdog.csv: {len(event_data_df)}\")\n",
    "\n",
    "# # Removing women's fights and fighters\n",
    "# womens_weight_classes = ['Strawweight'] # 'Flyweight', 'Bantamweight', 'Featherweight'\n",
    "# fighter_info_df = pd.read_csv('./data/fighter_info.csv')\n",
    "# cleaned_fighter_info_df = fighter_info_df[~fighter_info_df['Weight Class'].isin(womens_weight_classes)]\n",
    "# cleaned_fighter_info_df.to_csv('./data/fighter_info_men_only.csv', index=False)\n",
    "# print(f\"Cleaned dataset saved to {'./data/fighter_info_men_only.csv'}\")\n",
    "# event_data_df = pd.read_csv('./data/event_data_sherdog.csv')\n",
    "# cleaned_event_data_df = event_data_df[~event_data_df['Weight Class'].isin(womens_weight_classes)]\n",
    "# cleaned_event_data_df.to_csv('./data/event_data_sherdog_men_only.csv', index=False)\n",
    "# print(f\"Cleaned event dataset saved to {'./data/event_data_sherdog_men_only.csv'}\")\n",
    "# print(f\"Number of rows in fighter_info_men_only.csv: {len(cleaned_fighter_info_df)}\")\n",
    "# print(f\"Number of rows in event_data_sherdog_men_only.csv: {len(cleaned_event_data_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UFC\n",
    "##### [ufc.com](https://www.ufc.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UFC: Fighters\n",
    "\n",
    "# Sitemaps\n",
    "if os.path.exists(\"data/ufc_roster.csv\"): os.remove(\"data/ufc_roster.csv\")\n",
    "ns = {\n",
    "    \"s\": \"http://www.sitemaps.org/schemas/sitemap/0.9\",\n",
    "    \"xhtml\": \"http://www.w3.org/1999/xhtml\"\n",
    "}\n",
    "all_fighters = []\n",
    "page = 1\n",
    "consecutive_empty_pages = 0\n",
    "max_consecutive_empty = 100\n",
    "while True:\n",
    "    sitemap_url = f\"https://www.ufc.com/sitemap.xml?page={page}\"\n",
    "    try:\n",
    "        response = requests.get(sitemap_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"Page {page} does not exist. Stopping.\")\n",
    "        break\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching page {page}: {e}\")\n",
    "        page += 1\n",
    "        continue\n",
    "    try:\n",
    "        root = ET.fromstring(response.content)\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Error parsing page {page}: {e}\")\n",
    "        if \"not well-formed (invalid token)\" in str(e):\n",
    "            print(\"Reached end of valid pages. Stopping.\")\n",
    "            break\n",
    "        page += 1\n",
    "        continue\n",
    "    fighter_urls = []\n",
    "    for url_elem in root.findall(\"s:url\", ns):\n",
    "        loc_elem = url_elem.find(\"s:loc\", ns)\n",
    "        if loc_elem is not None:\n",
    "            url_text = loc_elem.text\n",
    "            if \"/athlete/\" in url_text:\n",
    "                fighter_urls.append(url_text)\n",
    "    print(f\"https://www.ufc.com/sitemap.xml?page={page} -> {len(fighter_urls)} fighter URLs\")\n",
    "    if len(fighter_urls) == 0:\n",
    "        consecutive_empty_pages += 1\n",
    "        if consecutive_empty_pages >= max_consecutive_empty:\n",
    "            print(f\"Found {max_consecutive_empty} consecutive empty pages. Stopping.\")\n",
    "            break\n",
    "    else:\n",
    "        consecutive_empty_pages = 0\n",
    "\n",
    "    for url in fighter_urls:\n",
    "        try:\n",
    "            slug = url.split(\"/athlete/\")[1]\n",
    "            name = slug.replace(\"-\", \" \").title()\n",
    "        except IndexError:\n",
    "            name = \"\"\n",
    "        all_fighters.append({\"name\": name, \"url\": url})\n",
    "    page += 1\n",
    "print(f\"Total fighter records found: {len(all_fighters)}\")\n",
    "csv_filename = \"data/ufc_roster.csv\"\n",
    "with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    fieldnames = [\"name\", \"url\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for fighter in all_fighters:\n",
    "        writer.writerow(fighter)\n",
    "print(f\"All fighter names and URLs saved to {csv_filename}\")\n",
    "df = pd.read_csv(\"data/ufc_roster.csv\")\n",
    "all_fighter_urls = df[\"url\"].tolist()\n",
    "print(f\"Total fighter URLs loaded from CSV: {len(all_fighter_urls)}\")\n",
    "\n",
    "\n",
    "# # Fighter Pages\n",
    "# if os.path.exists(\"data/ufc_fighter_profiles.csv\"): os.remove(\"data/ufc_fighter_profiles.csv\")\n",
    "# fighter_data = pd.DataFrame(columns=['name', 'url'])\n",
    "# for idx, url in enumerate(all_fighter_urls, start=1):\n",
    "#     print(f\"Scraping fighter {idx}/{len(all_fighter_urls)}: {url}\")\n",
    "#     resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "#     if resp.status_code != 200:\n",
    "#         print(f\"Error fetching fighter page: {url}\")\n",
    "#         continue\n",
    "#     soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "#     name_tag = soup.find(\"h1\")\n",
    "#     name = name_tag.get_text(strip=True) if name_tag else \"Unknown\"\n",
    "#     fighter_data.loc[len(fighter_data)] = [name, url]\n",
    "#     if idx % 100 == 0:\n",
    "#         fighter_data.to_csv(\"data/ufc_fighter_profiles.csv\", index=False)\n",
    "#         print(f\"Saved {len(fighter_data)} fighter profiles to data/ufc_fighter_profiles.csv\")\n",
    "# if len(fighter_data) > 0:\n",
    "#     fighter_data.to_csv(\"data/ufc_fighter_profiles.csv\", index=False)\n",
    "#     print(f\"Saved final {len(fighter_data)} fighter profiles to data/ufc_fighter_profiles.csv\")\n",
    "# roster_csv = \"data/ufc_roster.csv\"\n",
    "# if not os.path.exists(roster_csv):\n",
    "#     print(f\"Error: {roster_csv} not found. Run the roster scraper first.\")\n",
    "#     exit()\n",
    "# df = pd.read_csv(roster_csv)\n",
    "# print(f\"Columns in {roster_csv}: {df.columns.tolist()}\")\n",
    "# all_fighter_urls = df[\"url\"].dropna().tolist()\n",
    "# print(f\"Total fighter URLs loaded from CSV: {len(all_fighter_urls)}\")\n",
    "# profile_csv = \"data/ufc_fighter_profiles.csv\"\n",
    "# if os.path.exists(profile_csv):\n",
    "#     os.remove(profile_csv)\n",
    "# fieldnames = [\"name\", \"nickname\", \"weight_class\", \"profile_url\", \"thumbnail\"]\n",
    "# def scrape_fighter(url):\n",
    "#     \"\"\"\n",
    "#     Scrapes a fighter page and returns the extracted data as a dictionary.\n",
    "#     Implements basic retry logic in case of temporary rate limiting or non-200 responses.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         resp = None\n",
    "#         for attempt in range(3):\n",
    "#             resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "#             if resp.status_code == 200:\n",
    "#                 break\n",
    "#             elif resp.status_code == 429:\n",
    "#                 print(f\"Rate-limited. Waiting before retry... (Attempt {attempt + 1})\")\n",
    "#                 time.sleep(60)\n",
    "#             else:\n",
    "#                 print(f\"Error fetching fighter page {url} (status {resp.status_code}), attempt {attempt + 1} of 3.\")\n",
    "#                 time.sleep(10)\n",
    "#         if not resp or resp.status_code != 200:\n",
    "#             print(f\"Failed to fetch fighter page after retries: {url}\")\n",
    "#             return None\n",
    "#         soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "#         name_tag = soup.find(\"h1\", class_=\"hero-profile__name\")\n",
    "#         name = name_tag.get_text(strip=True) if name_tag else \"Unknown\"\n",
    "#         nickname_tag = soup.find(\"p\", class_=\"hero-profile__nickname\")\n",
    "#         nickname = nickname_tag.get_text(strip=True) if nickname_tag else \"\"\n",
    "#         weight_class_tag = soup.find(\"p\", class_=\"hero-profile__division-title\")\n",
    "#         weight_class = weight_class_tag.get_text(strip=True) if weight_class_tag else \"\"\n",
    "#         thumbnail_tag = soup.find(\"img\", class_=\"hero-profile__image\")\n",
    "#         thumbnail = thumbnail_tag[\"src\"] if thumbnail_tag and \"src\" in thumbnail_tag.attrs else \"\"\n",
    "#         fighter_data = {\n",
    "#             \"name\": name,\n",
    "#             \"nickname\": nickname,\n",
    "#             \"weight_class\": weight_class,\n",
    "#             \"profile_url\": url,\n",
    "#             \"thumbnail\": thumbnail\n",
    "#         }\n",
    "#         return fighter_data\n",
    "#     except Exception as e:\n",
    "#         print(f\"Exception occurred while scraping {url}: {e}\")\n",
    "#         return None\n",
    "# results = []\n",
    "# max_workers = 10\n",
    "# with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "#     future_to_url = {executor.submit(scrape_fighter, url): url for url in all_fighter_urls}\n",
    "#     for idx, future in enumerate(as_completed(future_to_url), start=1):\n",
    "#         data = future.result()\n",
    "#         if data:\n",
    "#             results.append(data)\n",
    "#         print(f\"Processed {idx} of {len(all_fighter_urls)} fighter profiles\")\n",
    "#         if idx % 100 == 0:\n",
    "#             print(f\"Processed {idx} fighter profiles\")\n",
    "# with open(profile_csv, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "#     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#     writer.writeheader()\n",
    "#     for fighter_data in results:\n",
    "#         writer.writerow(fighter_data)\n",
    "# print(f\"All fighter profiles saved to {profile_csv}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# roster_csv = \"data/ufc_roster.csv\"\n",
    "# df = pd.read_csv(roster_csv)\n",
    "# print(f\"Columns in {roster_csv}: {df.columns.tolist()}\")\n",
    "# all_fighter_urls = df[\"url\"].dropna().tolist()\n",
    "# print(f\"Total fighter URLs loaded from CSV: {len(all_fighter_urls)}\")\n",
    "# profile_csv = \"data/ufc_fighter_profiles.csv\"\n",
    "# if os.path.exists(\"data/ufc_fighter_profiles.csv\"): os.remove(\"data/ufc_fighter_profiles.csv\")\n",
    "# fieldnames = [\"name\", \"nickname\", \"weight_class\", \"profile_url\", \"thumbnail\"]\n",
    "# def scrape_fighter(url):\n",
    "#     \"\"\"\n",
    "#     Scrapes a fighter page and returns the extracted data as a dictionary.\n",
    "#     Implements basic retry logic in case of temporary rate limiting or non-200 responses.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         resp = None\n",
    "#         for attempt in range(3):\n",
    "#             resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "#             if resp.status_code == 200:\n",
    "#                 break\n",
    "#             elif resp.status_code == 429:\n",
    "#                 print(f\"Rate-limited. Waiting before retry... (Attempt {attempt + 1})\")\n",
    "#                 time.sleep(60)\n",
    "#             else:\n",
    "#                 print(f\"Error fetching fighter page {url} (status {resp.status_code}), attempt {attempt + 1} of 3.\")\n",
    "#                 time.sleep(10)\n",
    "#         if not resp or resp.status_code != 200:\n",
    "#             print(f\"Failed to fetch fighter page after retries: {url}\")\n",
    "#             return None\n",
    "#         soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "#         name_tag = soup.find(\"h1\", class_=\"hero-profile__name\")\n",
    "#         name = name_tag.get_text(strip=True) if name_tag else \"Unknown\"\n",
    "#         nickname_tag = soup.find(\"p\", class_=\"hero-profile__nickname\")\n",
    "#         nickname = nickname_tag.get_text(strip=True) if nickname_tag else \"\"\n",
    "#         weight_class_tag = soup.find(\"p\", class_=\"hero-profile__division-title\")\n",
    "#         weight_class = weight_class_tag.get_text(strip=True) if weight_class_tag else \"\"\n",
    "#         thumbnail_tag = soup.find(\"img\", class_=\"hero-profile__image\")\n",
    "#         thumbnail = thumbnail_tag[\"src\"] if thumbnail_tag and \"src\" in thumbnail_tag.attrs else \"\"\n",
    "#         fighter_data = {\n",
    "#             \"name\": name,\n",
    "#             \"nickname\": nickname,\n",
    "#             \"weight_class\": weight_class,\n",
    "#             \"profile_url\": url,\n",
    "#             \"thumbnail\": thumbnail\n",
    "#         }\n",
    "#         return fighter_data\n",
    "#     except Exception as e:\n",
    "#         print(f\"Exception occurred while scraping {url}: {e}\")\n",
    "#         return None\n",
    "# results = []\n",
    "# max_workers = 10\n",
    "# with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "#     future_to_url = {executor.submit(scrape_fighter, url): url for url in all_fighter_urls}\n",
    "#     for idx, future in enumerate(as_completed(future_to_url), start=1):\n",
    "#         data = future.result()\n",
    "#         if data:\n",
    "#             results.append(data)\n",
    "#         print(f\"Processed {idx} of {len(all_fighter_urls)} fighter profiles\")\n",
    "#         if idx % 100 == 0:\n",
    "#             with open(profile_csv, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "#                 writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#                 for fighter_data in results:\n",
    "#                     writer.writerow(fighter_data)\n",
    "#             results = []\n",
    "#             print(f\"Saved {idx} fighter profiles to {profile_csv}\")\n",
    "# # with open(profile_csv, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "# with open(profile_csv, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "#     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#     # writer.writeheader()\n",
    "#     for fighter_data in results:\n",
    "#         writer.writerow(fighter_data)\n",
    "# print(f\"All fighter profiles saved to {profile_csv}\")\n",
    "# # with open(profile_csv, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "# #     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "# #     for fighter_data in results:\n",
    "# #         writer.writerow(fighter_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "roster_csv = \"data/ufc_roster.csv\"\n",
    "df = pd.read_csv(roster_csv)\n",
    "print(f\"Columns in {roster_csv}: {df.columns.tolist()}\")\n",
    "all_fighter_urls = df[\"url\"].dropna().tolist()\n",
    "print(f\"Total fighter URLs loaded from CSV: {len(all_fighter_urls)}\")\n",
    "profile_csv = \"data/ufc_fighter_profiles.csv\"\n",
    "if os.path.exists(\"data/ufc_fighter_profiles.csv\"): os.remove(\"data/ufc_fighter_profiles.csv\")\n",
    "fieldnames = [\"name\", \"nickname\", \"weight_class\", \"profile_url\", \"thumbnail\"]\n",
    "def scrape_fighter(url):\n",
    "    \"\"\"\n",
    "    Scrapes a fighter page and returns the extracted data as a dictionary.\n",
    "    Implements basic retry logic in case of temporary rate limiting or non-200 responses.\n",
    "    \"\"\"\n",
    "    for attempt in range(1, 4):\n",
    "        try:\n",
    "            resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=15)\n",
    "            if resp.status_code == 200:\n",
    "                soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "                name_tag = soup.find(\"h1\", class_=\"hero-profile__name\")\n",
    "                name = name_tag.get_text(strip=True) if name_tag else \"Unknown\"\n",
    "                nickname_tag = soup.find(\"p\", class_=\"hero-profile__nickname\")\n",
    "                nickname = nickname_tag.get_text(strip=True) if nickname_tag else \"\"\n",
    "                weight_class_tag = soup.find(\"p\", class_=\"hero-profile__division-title\")\n",
    "                weight_class = weight_class_tag.get_text(strip=True) if weight_class_tag else \"\"\n",
    "                thumbnail_tag = soup.find(\"img\", class_=\"hero-profile__image\")\n",
    "                thumbnail = thumbnail_tag[\"src\"] if thumbnail_tag and \"src\" in thumbnail_tag.attrs else \"\"\n",
    "                fighter_data = {\n",
    "                    \"name\": name,\n",
    "                    \"nickname\": nickname,\n",
    "                    \"weight_class\": weight_class,\n",
    "                    \"profile_url\": url,\n",
    "                    \"thumbnail\": thumbnail\n",
    "                }\n",
    "                return fighter_data\n",
    "            elif resp.status_code == 429:\n",
    "                print(f\"Rate-limited. Waiting before retry... (Attempt {attempt} of 3)\")\n",
    "                time.sleep(60)\n",
    "            else:\n",
    "                print(f\"Error fetching fighter page {url} (status {resp.status_code}), attempt {attempt} of 3.\")\n",
    "                time.sleep(20)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occurred while scraping {url}: {e} (Attempt {attempt} of 3)\")\n",
    "            time.sleep(20)\n",
    "    print(f\"Failed to fetch fighter page after 3 retries: {url}\")\n",
    "    return None\n",
    "\n",
    "results = []\n",
    "max_workers = 10\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    future_to_url = {executor.submit(scrape_fighter, url): url for url in all_fighter_urls}\n",
    "    for idx, future in enumerate(as_completed(future_to_url), start=1):\n",
    "        data = future.result()\n",
    "        if data:\n",
    "            results.append(data)\n",
    "        print(f\"Processed {idx} of {len(all_fighter_urls)} fighter profiles\")\n",
    "        if idx % 100 == 0:\n",
    "            with open(profile_csv, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                for fighter_data in results:\n",
    "                    writer.writerow(fighter_data)\n",
    "            results = []\n",
    "            print(f\"Saved {idx} fighter profiles to {profile_csv}\")\n",
    "            time.sleep(10)  # Sleep for 10 seconds after every 100 saves\n",
    "# with open(profile_csv, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "with open(profile_csv, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    # writer.writeheader()\n",
    "    for fighter_data in results:\n",
    "        writer.writerow(fighter_data)\n",
    "print(f\"All fighter profiles saved to {profile_csv}\")\n",
    "# with open(profile_csv, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "#     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#     for fighter_data in results:\n",
    "#         writer.writerow(fighter_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GitHub\n",
    "##### [github.com/Greco1899/scrape_ufc_stats](https://github.com/Greco1899/scrape_ufc_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GitHub Data\n",
    "\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import csv\n",
    "# import warnings\n",
    "# import re\n",
    "# import sqlite3\n",
    "\n",
    "# os.makedirs('./data/github/', exist_ok=True)\n",
    "# os.makedirs('./data/github/fighter-details', exist_ok=True)\n",
    "\n",
    "# urls = [\n",
    "#     'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/ufc_event_details.csv',\n",
    "#     'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/ufc_fight_details.csv',\n",
    "#     'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/ufc_fight_results.csv',\n",
    "#     'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/ufc_fight_stats.csv',\n",
    "#     'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/ufc_fighter_details.csv',\n",
    "#     'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/ufc_fighter_tott.csv'\n",
    "# ]\n",
    "\n",
    "# for url in urls:\n",
    "#     df = pd.read_csv(url)\n",
    "#     df = df.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "#     df.to_csv('./data/github/' + url.split('/')[-1], index=False)\n",
    "    \n",
    "# # Merge the DATE and LOCATION of every fight\n",
    "\n",
    "# event_details_df = pd.read_csv('./data/github/ufc_event_details.csv')\n",
    "# fight_results_df = pd.read_csv('./data/github/ufc_fight_results.csv')\n",
    "\n",
    "# # Set the 'EVENT' column as index in the event details DataFrame for easier merging\n",
    "# event_details_df_for_merge = event_details_df.set_index('EVENT')\n",
    "\n",
    "# # Merge 'DATE' and 'LOCATION' from event details into fight results DataFrame based on 'EVENT'\n",
    "# merged_df = fight_results_df.join(event_details_df_for_merge[['DATE', 'LOCATION']], on='EVENT')\n",
    "\n",
    "# merged_df.to_csv('./data/github/master.csv', index=False)\n",
    "\n",
    "# # Create FIGHTER1, FIGHTER2, and WINNING_FIGHTER columns\n",
    "\n",
    "# fight_results_df = pd.read_csv('./data/github/master.csv')\n",
    "\n",
    "# # Split the 'BOUT' column into two new columns 'Fighter1' and 'Fighter2'\n",
    "# fight_results_df[['FIGHTER1', 'FIGHTER2']] = fight_results_df['BOUT'].str.split(' vs. ', expand=True)\n",
    "\n",
    "# # Use a multiline lambda within apply for readability without an external function\n",
    "# fight_results_df['WINNING_FIGHTER'] = fight_results_df.apply(\n",
    "#     lambda row: row['FIGHTER1'].strip() if row['OUTCOME'] == 'W/L' else\n",
    "#                 (row['FIGHTER2'].strip() if row['OUTCOME'] == 'L/W' else\n",
    "#                  ('No Contest' if row['OUTCOME'] == 'NC/NC' else\n",
    "#                   ('Draw' if row['OUTCOME'] == 'D/D' else 'Unknown Outcome'))),\n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# fight_results_df.to_csv('./data/github/master.csv', index=False)\n",
    "\n",
    "# # Load the latest master.csv file into a DataFrame\n",
    "# latest_master_df = pd.read_csv('./data/github/master.csv')\n",
    "\n",
    "# # Clean leading and trailing whitespace in FIGHTER1 and FIGHTER2 columns\n",
    "# latest_master_df['FIGHTER1'] = latest_master_df['FIGHTER1'].str.strip()\n",
    "# latest_master_df['FIGHTER2'] = latest_master_df['FIGHTER2'].str.strip()\n",
    "\n",
    "# # Create an SQLite database connection in a writable location\n",
    "# conn = sqlite3.connect('ufc_database.db')\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# # Create the fight_results table\n",
    "# create_table_query = \"\"\"\n",
    "# CREATE TABLE IF NOT EXISTS fight_results (\n",
    "#     EVENT TEXT,\n",
    "#     BOUT TEXT,\n",
    "#     OUTCOME TEXT,\n",
    "#     WEIGHTCLASS TEXT,\n",
    "#     METHOD TEXT,\n",
    "#     ROUND INTEGER,\n",
    "#     TIME TEXT,\n",
    "#     TIME_FORMAT TEXT,\n",
    "#     REFEREE TEXT,\n",
    "#     DETAILS TEXT,\n",
    "#     URL TEXT,\n",
    "#     DATE TEXT,\n",
    "#     LOCATION TEXT,\n",
    "#     FIGHTER1 TEXT,\n",
    "#     FIGHTER2 TEXT,\n",
    "#     WINNING_FIGHTER TEXT\n",
    "# )\n",
    "# \"\"\"\n",
    "# cursor.execute(create_table_query)\n",
    "# conn.commit()\n",
    "\n",
    "# # Insert data into the fight_results table\n",
    "# latest_master_df.to_sql('fight_results', conn, if_exists='replace', index=False)\n",
    "\n",
    "# # Verify the data insertion\n",
    "# result = cursor.execute(\"SELECT COUNT(*) FROM fight_results\").fetchone()[0]\n",
    "\n",
    "# # Close the connection\n",
    "# conn.close()\n",
    "\n",
    "# result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!open data/event_data_sherdog.csv\n",
    "!open data/fighter_info.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
