{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf0a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2861f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().system('rm -rf data-raw')\n",
    "\n",
    "directory_path = './data-raw/'\n",
    "for root, dirs, files in os.walk(directory_path, topdown=False):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        os.remove(file_path)\n",
    "    for dir in dirs:\n",
    "        dir_path = os.path.join(root, dir)\n",
    "        os.rmdir(dir_path)\n",
    "\n",
    "directories = ['./data-raw/fighters/', './data-raw/git/']\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "csv_files = ['./data-raw/event_urls_sherdog.csv', './data-raw/event_data_sherdog.csv', './data-raw/fighter_id_sherdog.csv', './data-raw/fighter_info.csv']\n",
    "for file_path in csv_files:\n",
    "    if not os.path.exists(file_path):\n",
    "        open(file_path, 'a').close()\n",
    "\n",
    "files = [\"ufc_event_details.csv\", \"ufc_fight_results.csv\", \"ufc_fight_stats.csv\", \"ufc_fighter_tott.csv\"]\n",
    "base_url = \"https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/\"\n",
    "directory = \"./data-raw/git/\"\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "for file in files:\n",
    "    response = requests.get(base_url + file)\n",
    "    if response.status_code == 200:\n",
    "        with open(os.path.join(directory, file), 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(f\"Failed to download {file}\")\n",
    "\n",
    "columns = ['Event_URL']\n",
    "file_path = './data-raw/event_urls_sherdog.csv'\n",
    "if os.path.isfile(file_path):\n",
    "    if os.path.getsize(file_path) == 0:\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        df.to_csv(file_path, index=False)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "columns = ['Fighter', 'Fighter_ID']\n",
    "file_path = './data-raw/fighter_id_sherdog.csv'\n",
    "if os.path.isfile(file_path):\n",
    "    if os.path.getsize(file_path) == 0:\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        df.to_csv(file_path, index=False)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "columns = ['Event Name', 'Event Location', 'Event Date', 'Fighter 1', 'Fighter 2', 'Weight Class', 'Winning Fighter', 'Winning Method', 'Winning Round', 'Winning Time', 'Referee']\n",
    "file_path = './data-raw/event_data_sherdog.csv'\n",
    "if os.path.isfile(file_path):\n",
    "    if os.path.getsize(file_path) == 0:\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        df.to_csv(file_path, index=False)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "columns = ['Fighter', 'Nickname', 'Birth Date', 'Nationality', 'Hometown', 'Association', 'Weight Class', 'Height', 'REACH', 'STANCE', 'Wins', 'Losses', 'Win_Decision', 'Win_KO', 'Win_Sub', 'Loss_Decision', 'Loss_KO', 'Loss_Sub', 'Sherdog URL', 'BFO URL']\n",
    "file_path = './data-raw/fighter_info.csv'\n",
    "if os.path.isfile(file_path):\n",
    "    if os.path.getsize(file_path) == 0:\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        df.to_csv(file_path, index=False)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def scrape_event_urls_sherdog():\n",
    "    file_path = './data-raw/event_urls_sherdog.csv'\n",
    "    urls = [\n",
    "        'https://www.sherdog.com/organizations/Ultimate-Fighting-Championship-UFC-2/recent-events/1',\n",
    "        'https://www.sherdog.com/organizations/Ultimate-Fighting-Championship-UFC-2/recent-events/2',\n",
    "        'https://www.sherdog.com/organizations/Ultimate-Fighting-Championship-UFC-2/recent-events/3',\n",
    "        'https://www.sherdog.com/organizations/Ultimate-Fighting-Championship-UFC-2/recent-events/4',\n",
    "        'https://www.sherdog.com/organizations/Ultimate-Fighting-Championship-UFC-2/recent-events/5',\n",
    "        'https://www.sherdog.com/organizations/Ultimate-Fighting-Championship-UFC-2/recent-events/6',\n",
    "        'https://www.sherdog.com/organizations/Ultimate-Fighting-Championship-UFC-2/recent-events/7'\n",
    "    ]\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    if os.path.isfile(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=['Event_URL'])\n",
    "\n",
    "    progress_bar = tqdm(urls, desc=\"Scraping URLs\", unit=\"URL\",\n",
    "                        bar_format=\"{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {postfix}]\")\n",
    "\n",
    "    for index, url in enumerate(progress_bar, start=1):\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        specific_div = soup.find('div', {'class': 'single_tab', 'id': 'recent_tab'})\n",
    "        new_urls = []\n",
    "        for a in specific_div.find_all('a', itemprop='url'):\n",
    "            href = a.get('href')\n",
    "            if href and href not in df['Event_URL'].values:\n",
    "                new_urls.append(href)\n",
    "\n",
    "        new_urls_df = pd.DataFrame(new_urls, columns=['Event_URL'])\n",
    "        df = pd.concat([df, new_urls_df], ignore_index=True)\n",
    "\n",
    "        progress_bar.set_postfix({\"Total URLs\": len(df), \"Current URL\": url})\n",
    "\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(df)\n",
    "    \n",
    "scrape_event_urls_sherdog()\n",
    "\n",
    "urls_to_delete = [\n",
    "    \"/events/UFC-233-Ultimate-Fighting-Championship-233-72021\",\n",
    "    \"/events/UFC-Fight-Night-97-Lamas-vs-Penn-90890\",\n",
    "    \"/events/UFC-176-Aldo-vs-Mendes-2-37609\",\n",
    "    \"/events/UFC-151-Jones-vs-Henderson-25809\"\n",
    "]\n",
    "\n",
    "input_file = \"./data-raw/event_urls_sherdog.csv\"\n",
    "output_file = \"./data-raw/event_urls_sherdog.csv\"\n",
    "\n",
    "with open(input_file, \"r\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    rows = [row for row in reader if row[\"Event_URL\"] not in urls_to_delete]\n",
    "\n",
    "fieldnames = [\"Event_URL\", \"Event Date\"]\n",
    "\n",
    "with open(output_file, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(\"URLs deleted successfully.\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def fetch_event_date(session, url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "    try:\n",
    "        response = session.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        event_date_meta = soup.find('meta', itemprop='startDate')\n",
    "        if event_date_meta and 'content' in event_date_meta.attrs:\n",
    "            return event_date_meta['content'].strip()\n",
    "        else:\n",
    "            return 'Not Found'\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing URL {url}: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "def scrape_event_date():\n",
    "    df = pd.read_csv('./data-raw/event_urls_sherdog.csv')\n",
    "    df['Event_URL'] = 'https://sherdog.com' + df['Event_URL']\n",
    "\n",
    "    event_dates = []\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=40) as executor:\n",
    "            future_to_url = {executor.submit(fetch_event_date, session, url): url for url in df['Event_URL']}\n",
    "            for future in tqdm(concurrent.futures.as_completed(future_to_url), total=len(df['Event_URL']), desc=\"Scraping Events\", unit=\"Event\"):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    event_date = future.result()\n",
    "                    event_dates.append(event_date)\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while processing URL {url}: {e}\")\n",
    "                    event_dates.append(\"Error\")\n",
    "\n",
    "    df['Event Date'] = event_dates\n",
    "\n",
    "    df.to_csv('./data-raw/event_urls_sherdog.csv', index=False)\n",
    "\n",
    "    print(\"Event dates appended successfully.\")\n",
    "\n",
    "scrape_event_date()\n",
    "\n",
    "df = pd.read_csv('./data-raw/event_urls_sherdog.csv')\n",
    "print(f\"Total number of rows including the header in event_urls_sherdog.csv: {len(df)}\")\n",
    "print(f\"Column names: {list(df.columns)}\")\n",
    "\n",
    "file_path = './data-raw/event_urls_sherdog.csv'\n",
    "specified_line_to_remove = \"/events/UFC-302-June-29-101243\"\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "filtered_lines = [line for line in lines if specified_line_to_remove not in line]\n",
    "\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    for line in filtered_lines:\n",
    "        file.write(line)\n",
    "\n",
    "print(\"Specified line has been removed.\")\n",
    "\n",
    "get_ipython().system('rm ./data-raw/event_data_sherdog.csv')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "}\n",
    "\n",
    "urls_df = pd.read_csv('data-raw/event_urls_sherdog.csv')\n",
    "all_data = []\n",
    "\n",
    "def fetch_event_data(url, session):\n",
    "    full_url = f'https://sherdog.com{url}' if not url.startswith('http') else url\n",
    "    event_data = []\n",
    "    try:\n",
    "        with session.get(full_url, headers=headers) as response:\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                event_name = soup.find('span', itemprop='name').text.strip()\n",
    "                event_location = soup.find('span', itemprop='location').text.strip()\n",
    "                event_date = soup.find('meta', itemprop='startDate')['content'].strip()\n",
    "\n",
    "                main_event_fighters = soup.find_all('div', class_='fighter')\n",
    "                if main_event_fighters:\n",
    "                    fighter1 = main_event_fighters[0].find('span', itemprop='name').text.strip()\n",
    "                    fighter2 = main_event_fighters[1].find('span', itemprop='name').text.strip()\n",
    "                    fighter1_id = main_event_fighters[0].find('a', itemprop='url')['href'].split('-')[-1]\n",
    "                    fighter2_id = main_event_fighters[1].find('a', itemprop='url')['href'].split('-')[-1]\n",
    "                    weight_class = soup.find('span', class_='weight_class').text.strip()\n",
    "                    winning_fighter = fighter1\n",
    "                    winning_method_em = soup.find('em', string='Method').parent\n",
    "                    winning_method = winning_method_em.contents[2].strip()\n",
    "                    winning_round_em = soup.find('em', string='Round').parent\n",
    "                    winning_round = winning_round_em.contents[2].strip()\n",
    "                    winning_time_em = soup.find('em', string='Time').parent\n",
    "                    winning_time = winning_time_em.contents[2].strip()\n",
    "                    referee_em = soup.find('em', string='Referee').parent\n",
    "                    referee = referee_em.find('a').text.strip()\n",
    "                    event_data.append({\n",
    "                        'Event Name': event_name,\n",
    "                        'Event Location': event_location,\n",
    "                        'Event Date': event_date,\n",
    "                        'Fighter 1': fighter1,\n",
    "                        'Fighter 2': fighter2,\n",
    "                        'Fighter 1 ID': fighter1_id,\n",
    "                        'Fighter 2 ID': fighter2_id,\n",
    "                        'Weight Class': weight_class,\n",
    "                        'Winning Fighter': winning_fighter,\n",
    "                        'Winning Method': winning_method,\n",
    "                        'Winning Round': winning_round,\n",
    "                        'Winning Time': winning_time,\n",
    "                        'Referee': referee,\n",
    "                        'Fight Type': 'Main Event'\n",
    "                    })\n",
    "                    \n",
    "                other_bouts = soup.find_all('tr', itemprop='subEvent')\n",
    "                for bout in other_bouts:\n",
    "                    fighters = bout.find_all('div', class_='fighter_list')\n",
    "                    if len(fighters) >= 2:\n",
    "                        fighter1 = fighters[0].find('img')['title']\n",
    "                        fighter2 = fighters[1].find('img')['title']\n",
    "                        fighter1_url = fighters[0].find('a', itemprop='url')['href']\n",
    "                        fighter2_url = fighters[1].find('a', itemprop='url')['href']\n",
    "                        fighter1_id = fighter1_url.split('-')[-1]\n",
    "                        fighter2_id = fighter2_url.split('-')[-1]\n",
    "                        weight_class = bout.find('span', class_='weight_class')\n",
    "                        weight_class = weight_class.text.strip() if weight_class else \"Unknown\"\n",
    "                        winning_method = bout.find('td', class_='winby').find('b').get_text(strip=True)\n",
    "                        winning_round = bout.find_all('td')[-2].get_text(strip=True)\n",
    "                        winning_time = bout.find_all('td')[-1].get_text(strip=True)\n",
    "                        referee = bout.find('td', class_='winby').find('a').get_text(strip=True)\n",
    "                        event_data.append({\n",
    "                            'Event Name': event_name,\n",
    "                            'Event Location': event_location,\n",
    "                            'Event Date': event_date,\n",
    "                            'Fighter 1': fighter1,\n",
    "                            'Fighter 2': fighter2,\n",
    "                            'Fighter 1 ID': fighter1_id,\n",
    "                            'Fighter 2 ID': fighter2_id,\n",
    "                            'Weight Class': weight_class,\n",
    "                            'Winning Fighter': fighter1,\n",
    "                            'Winning Method': winning_method,\n",
    "                            'Winning Round': winning_round,\n",
    "                            'Winning Time': winning_time,\n",
    "                            'Referee': referee,\n",
    "                            'Fight Type': 'Undercard'\n",
    "                        })\n",
    "                        \n",
    "        return event_data\n",
    "    except Exception as e:\n",
    "        print(f\"Request failed for {full_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "session = requests.Session()\n",
    "total_urls = len(urls_df['Event_URL'])\n",
    "completed_requests = 0\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:\n",
    "    futures = [executor.submit(fetch_event_data, url, session) for url in urls_df['Event_URL']]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        data = future.result()\n",
    "        completed_requests += 1\n",
    "        progress_percentage = (completed_requests / total_urls) * 100\n",
    "        print(f\"Completed {completed_requests}/{total_urls} requests ({progress_percentage:.2f}%)\")\n",
    "        if data:\n",
    "            all_data.extend(data)\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "file_path = './data-raw/event_data_sherdog.csv'\n",
    "write_mode = 'a' if os.path.isfile(file_path) else 'w'\n",
    "\n",
    "df.to_csv(file_path, mode=write_mode, header=not os.path.isfile(file_path), index=False)\n",
    "\n",
    "df = pd.read_csv('./data-raw/event_data_sherdog.csv')\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "df.to_csv('./data-raw/event_data_sherdog.csv', index=False)\n",
    "\n",
    "df = pd.read_csv('./data-raw/event_data_sherdog.csv')\n",
    "print(f\"Total number of rows including the header in event_data_sherdog.csv: {len(df)}\")\n",
    "print(f\"Column names: {list(df.columns)}\")\n",
    "\n",
    "df = pd.read_csv('./data-raw/event_data_sherdog.csv')\n",
    "df2 = pd.read_csv('./data-raw/fighter_id_sherdog.csv')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    fighter1 = row['Fighter 1']\n",
    "    fighter2 = row['Fighter 2']\n",
    "    fighter1_id = row['Fighter 1 ID']\n",
    "    fighter2_id = row['Fighter 2 ID']\n",
    "    for fighter, fighter_id in zip([fighter1, fighter2], [fighter1_id, fighter2_id]):\n",
    "        if fighter not in df2['Fighter'].values and fighter_id not in df2['Fighter_ID'].values:\n",
    "            df2 = pd.concat([df2, pd.DataFrame([{'Fighter': fighter, 'Fighter_ID': fighter_id}])])\n",
    "\n",
    "df2.to_csv('./data-raw/fighter_id_sherdog.csv', index=False)\n",
    "\n",
    "def remove_nickname(name):\n",
    "    return re.sub(r\" '.+?'\", \"\", name)\n",
    "\n",
    "df = pd.read_csv('./data-raw/fighter_id_sherdog.csv')\n",
    "df['Fighter'] = df['Fighter'].apply(remove_nickname)\n",
    "df.to_csv('./data-raw/fighter_id_sherdog.csv', index=False)\n",
    "\n",
    "df = pd.read_csv('./data-raw/event_data_sherdog.csv')\n",
    "df['Fighter 1'] = df['Fighter 1'].apply(remove_nickname)\n",
    "df.to_csv('./data-raw/event_data_sherdog.csv', index=False)\n",
    "\n",
    "df = pd.read_csv('./data-raw/event_data_sherdog.csv')\n",
    "df['Fighter 2'] = df['Fighter 2'].apply(remove_nickname)\n",
    "df.to_csv('./data-raw/event_data_sherdog.csv', index=False)\n",
    "\n",
    "df = pd.read_csv('./data-raw/event_data_sherdog.csv')\n",
    "df['Winning Fighter'] = df['Winning Fighter'].apply(remove_nickname)\n",
    "df.to_csv('./data-raw/event_data_sherdog.csv', index=False)\n",
    "\n",
    "input_file_path = './data-raw/fighter_id_sherdog.csv'\n",
    "output_file_path = './data-raw/fighter_id_sherdog.csv'\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "df['UFC'] = 'y'\n",
    "\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(\"New column 'UFC' added to the CSV file.\")\n",
    "\n",
    "df = pd.read_csv('./data-raw/fighter_id_sherdog.csv')\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "df.to_csv('./data-raw/fighter_id_sherdog.csv', index=False)\n",
    "\n",
    "df = pd.read_csv('./data-raw/fighter_id_sherdog.csv')\n",
    "print(f\"Total number of rows including the header in fighter_id_sherdog.csv: {len(df)}\")\n",
    "print(f\"Column names: {list(df.columns)}\")\n",
    "\n",
    "import warnings\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tqdm\n",
    "\n",
    "def scrape_fighter_general_info_sherdog(fighter, fighter_id):\n",
    "    url = f'https://www.sherdog.com/fighter/{fighter_id}'\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return {}\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    fighter_dict = {}\n",
    "    try:\n",
    "        fighter_data = soup.find('div', class_='fighter-data')\n",
    "    except AttributeError:\n",
    "        fighter_data = None\n",
    "    try:\n",
    "        birthdate = soup.find('span', itemprop='birthDate')\n",
    "        birthdate = (birthdate.text).strip('\"\"')\n",
    "    except AttributeError:\n",
    "        birthdate = '-'\n",
    "    try:\n",
    "        nationality = soup.find('strong', itemprop='nationality')\n",
    "        nationality = (nationality.text).strip()\n",
    "    except AttributeError:\n",
    "        nationality = '-'\n",
    "    try:\n",
    "        hometown = soup.find('span', {'itemprop': 'addressLocality'}).text\n",
    "        hometown = hometown.strip()\n",
    "    except AttributeError:\n",
    "        hometown = '-'\n",
    "    try:\n",
    "        association = soup.find('span', {'itemprop': 'name'}).text\n",
    "        association = association.strip()\n",
    "    except AttributeError:\n",
    "        association = '-'\n",
    "    try:\n",
    "        weight_class_div = fighter_data.find('div', {'class': 'association-class'})\n",
    "        links = weight_class_div.find_all('a')\n",
    "        weight_class = links[-1].text\n",
    "        weight_class = weight_class.strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        weight_class = ''\n",
    "    try:\n",
    "        nickname = soup.find('span', class_='nickname')\n",
    "        nickname = (nickname.text).strip('\"')\n",
    "    except AttributeError:\n",
    "        nickname = '-'\n",
    "    try:\n",
    "        height = soup.find('b', itemprop='height')\n",
    "        height = (height.text).strip('\"')\n",
    "    except AttributeError:\n",
    "        height = '-'\n",
    "    try:\n",
    "        wins = soup.find('div', class_='winloses win').find_all('span')[1]\n",
    "        wins = (wins.text).strip()\n",
    "    except AttributeError:\n",
    "        wins = '-'\n",
    "    try:\n",
    "        losses = soup.find('div', class_='winloses lose').find_all('span')[1]\n",
    "        losses = (losses.text).strip()\n",
    "    except AttributeError:\n",
    "        losses = '-'\n",
    "    dec_data_list = []\n",
    "    try:\n",
    "        win_type = fighter_data.find_all('div', class_='meter-title', string='DECISIONS')\n",
    "        for method in win_type:\n",
    "            if method.text.startswith('DECISIONS'):\n",
    "                dec_data = method.find_next('div', class_='pl').text\n",
    "                dec_data_list.append(dec_data)\n",
    "        wins_dec = (dec_data_list[0]).strip()\n",
    "        losses_dec = (dec_data_list[1]).strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        wins_dec = '-'\n",
    "        losses_dec = '-'\n",
    "    ko_data_list = []\n",
    "    try:\n",
    "        win_type = soup.find_all('div', class_='meter-title')\n",
    "        for method in win_type:\n",
    "            if method.text.startswith('KO'):\n",
    "                ko_data = method.find_next('div', class_='pl').text\n",
    "                ko_data_list.append(ko_data)\n",
    "        wins_ko = (ko_data_list[0]).strip()\n",
    "        losses_ko = (ko_data_list[1]).strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        wins_ko = '-'\n",
    "        losses_ko = '-'\n",
    "    sub_data_list = []\n",
    "    try:\n",
    "        win_type = fighter_data.find_all('div', class_='meter-title', string='SUBMISSIONS')\n",
    "        for method in win_type:\n",
    "            if method.text.startswith('SUBMISSIONS'):\n",
    "                sub_data = method.find_next('div', class_='pl').text\n",
    "                sub_data_list.append(sub_data)\n",
    "        wins_sub = (sub_data_list[0]).strip()\n",
    "        losses_sub = (sub_data_list[1]).strip()\n",
    "    except (AttributeError, IndexError):\n",
    "        wins_sub = '-'\n",
    "        losses_sub = '-'\n",
    "    fighter_dict = {\n",
    "        'Fighter': fighter,\n",
    "        'Nickname': nickname,\n",
    "        'Birth Date': birthdate,\n",
    "        'Nationality': nationality,\n",
    "        'Hometown': hometown,\n",
    "        'Association': association,\n",
    "        'Weight Class': weight_class,\n",
    "        'Height': height,\n",
    "        'Wins': wins,\n",
    "        'Losses': losses,\n",
    "        'Win_Decision': wins_dec,\n",
    "        'Win_KO': wins_ko,\n",
    "        'Win_Sub': wins_sub,\n",
    "        'Loss_Decision': losses_dec,\n",
    "        'Loss_KO': losses_ko,\n",
    "        'Loss_Sub': losses_sub,\n",
    "        'Fighter_ID': fighter_id\n",
    "    }\n",
    "    return fighter_dict\n",
    "\n",
    "def scrape_fighters_concurrently():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    df_fighter_id = pd.read_csv('./data-raw/fighter_id_sherdog.csv')\n",
    "    fighter_data_list = []\n",
    "    with ThreadPoolExecutor(max_workers=30) as executor:\n",
    "        future_to_fighter = {executor.submit(scrape_fighter_general_info_sherdog, row['Fighter'], row['Fighter_ID']): row for index, row in df_fighter_id.iterrows()}\n",
    "        for future in tqdm.tqdm(as_completed(future_to_fighter), total=len(future_to_fighter)):\n",
    "            fighter_data = future.result()\n",
    "            if fighter_data:\n",
    "                fighter_data_list.append(fighter_data)\n",
    "    new_df = pd.DataFrame(fighter_data_list)\n",
    "    new_df.to_csv('./data-raw/fighter_info.csv', index=False)\n",
    "\n",
    "scrape_fighters_concurrently()\n",
    "\n",
    "df = pd.read_csv('./data-raw/fighter_info.csv')\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "df.to_csv('./data-raw/fighter_info.csv', index=False)\n",
    "\n",
    "df = pd.read_csv('./data-raw/fighter_info.csv')\n",
    "print(f\"Total number of rows including the header in fighter_info.csv: {len(df)}\")\n",
    "print(f\"Column names: {list(df.columns)}\")\n",
    "\n",
    "get_ipython().system('rm -rf ./data-raw/fighters/')\n",
    "\n",
    "directory_path = './data-raw/fighters/'\n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def scrape_fighter_fights_sherdog(fighter_name, fighter_id, fighter_url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    response = requests.get(fighter_url, headers=headers, timeout=60)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', {'class': 'new_table fighter'})\n",
    "        rows = table.find_all('tr')[1:]\n",
    "        fight_data = []\n",
    "        new_opponents = []\n",
    "\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            fight_dict = {\n",
    "                'Result': cols[0].text.strip(),\n",
    "                'Opponent': cols[1].find('a').text.strip() if cols[1].find('a') else '-',\n",
    "                'Event Date': cols[2].find_all('span')[-1].text.strip() if cols[2].find_all('span') else '-',\n",
    "                'Method/Referee': cols[3].text.strip().split('\\n')[0],\n",
    "                'Rounds': cols[4].text.strip(),\n",
    "                'Time': cols[5].text.strip()\n",
    "            }\n",
    "            fight_data.append(fight_dict)\n",
    "            opponent_link = cols[1].find('a')['href'] if cols[1].find('a') else None\n",
    "            if opponent_link:\n",
    "                opponent_id = opponent_link.split('-')[-1]\n",
    "                new_opponents.append({'Fighter': fight_dict['Opponent'], 'Fighter_ID': opponent_id})\n",
    "        return fighter_id, fighter_name, fight_data, new_opponents\n",
    "    return fighter_id, fighter_name, [], []\n",
    "\n",
    "df_fighter_id = pd.read_csv('./data-raw/fighter_id_sherdog.csv')\n",
    "all_new_opponents = []\n",
    "\n",
    "def process_fighter(row):\n",
    "    fighter_url = f\"https://www.sherdog.com/fighter/{row['Fighter'].replace(' ', '-')}-{row['Fighter_ID']}\"\n",
    "    return scrape_fighter_fights_sherdog(row['Fighter'], row['Fighter_ID'], fighter_url)\n",
    "\n",
    "total_fighters = len(df_fighter_id)\n",
    "fighters_processed = 0\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    futures = [executor.submit(process_fighter, row) for _, row in df_fighter_id.iterrows()]\n",
    "    for future in as_completed(futures):\n",
    "        fighter_id, fighter_name, fight_data, new_opponents = future.result()\n",
    "        fighters_processed += 1\n",
    "        print(f\"Processed {fighters_processed}/{total_fighters} fighters.\")\n",
    "        if fight_data:\n",
    "            pd.DataFrame(fight_data).to_csv(f\"./data-raw/fighters/{fighter_name.replace(' ', '_')}_{fighter_id}.csv\", index=False)\n",
    "            all_new_opponents.extend(new_opponents)\n",
    "\n",
    "if all_new_opponents:\n",
    "    df_new_opponents = pd.DataFrame(all_new_opponents).drop_duplicates()\n",
    "    df_fighter_id = pd.concat([df_fighter_id, df_new_opponents], ignore_index=True).drop_duplicates()\n",
    "    df_fighter_id.to_csv('./data-raw/fighter_id_sherdog.csv', index=False)\n",
    "\n",
    "directory_path = './data-raw/fighters/'\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        df.to_csv(file_path, index=False)\n",
    "\n",
    "first_name = \"Dustin\"\n",
    "last_name = \"Poirier\"\n",
    "\n",
    "file_pattern = f'./data-raw/fighters/*{first_name}*{last_name}*.csv'\n",
    "\n",
    "matching_files = glob.glob(file_pattern)\n",
    "\n",
    "if matching_files:\n",
    "    first_file = matching_files[0]\n",
    "    df = pd.read_csv(first_file)\n",
    "    \n",
    "    print(f\"Total number of rows including the header in {first_file}: {len(df)}\")\n",
    "    print(f\"Column names: {list(df.columns)}\")\n",
    "else:\n",
    "    print(\"No files found matching the pattern.\")\n",
    "\n",
    "fighter_info_df = pd.read_csv('./data-raw/fighter_info.csv')\n",
    "print(f\"Number of rows in fighter_info.csv: {len(fighter_info_df)}\")\n",
    "\n",
    "fighter_info_df = pd.read_csv('./data-raw/fighter_info.csv')\n",
    "print(f\"Number of rows in fighter_info.csv: {len(fighter_info_df)}\")\n",
    "\n",
    "womens_weight_classes = ['Strawweight']\n",
    "fighter_info_df = pd.read_csv('./data-raw/fighter_info.csv')\n",
    "cleaned_fighter_info_df = fighter_info_df[~fighter_info_df['Weight Class'].isin(womens_weight_classes)]\n",
    "cleaned_fighter_info_df.to_csv('./data-raw/fighter_info.csv', index=False)\n",
    "print(f\"Cleaned dataset saved to {'./data-raw/fighter_info.csv'}\")\n",
    "\n",
    "fighter_info_df = pd.read_csv('./data-raw/fighter_info.csv')\n",
    "print(f\"Number of rows in fighter_info.csv: {len(fighter_info_df)}\")\n",
    "\n",
    "get_ipython().system('rm -rf ./data/github')\n",
    "os.makedirs('./data/github/', exist_ok=True)\n",
    "os.makedirs('./data/github/fighter-details', exist_ok=True)\n",
    "urls = [\n",
    "    'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/ufc_event_details.csv',\n",
    "    'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/ufc_fight_details.csv',\n",
    "    'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/ufc_fight_results.csv',\n",
    "    'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/ufc_fight_stats.csv',\n",
    "    'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/ufc_fighter_details.csv',\n",
    "    'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/ufc_fighter_tott.csv'\n",
    "]\n",
    "for url in urls:\n",
    "    df = pd.read_csv(url)\n",
    "    df = df.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    df.to_csv('./data/github/' + url.split('/')[-1], index=False)\n",
    "    \n",
    "event_details_df = pd.read_csv('./data/github/ufc_event_details.csv')\n",
    "fight_results_df = pd.read_csv('./data/github/ufc_fight_results.csv')\n",
    "\n",
    "event_details_df_for_merge = event_details_df.set_index('EVENT')\n",
    "\n",
    "merged_df = fight_results_df.join(event_details_df_for_merge[['DATE', 'LOCATION']], on='EVENT')\n",
    "\n",
    "merged_df.to_csv('./data/github/master.csv', index=False)\n",
    "\n",
    "fight_results_df = pd.read_csv('./data/github/master.csv')\n",
    "\n",
    "fight_results_df[['FIGHTER1', 'FIGHTER2']] = fight_results_df['BOUT'].str.split(' vs. ', expand=True)\n",
    "\n",
    "fight_results_df['WINNING_FIGHTER'] = fight_results_df.apply(\n",
    "    lambda row: row['FIGHTER1'].strip() if row['OUTCOME'] == 'W/L' else\n",
    "                (row['FIGHTER2'].strip() if row['OUTCOME'] == 'L/W' else\n",
    "                 ('No Contest' if row['OUTCOME'] == 'NC/NC' else\n",
    "                  ('Draw' if row['OUTCOME'] == 'D/D' else 'Unknown Outcome'))),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "fight_results_df.to_csv('./data/github/master.csv', index=False)\n",
    "\n",
    "latest_master_df = pd.read_csv('./data/github/master.csv')\n",
    "\n",
    "latest_master_df['FIGHTER1'] = latest_master_df['FIGHTER1'].str.strip()\n",
    "latest_master_df['FIGHTER2'] = latest_master_df['FIGHTER2'].str.strip()\n",
    "\n",
    "conn = sqlite3.connect('ufc_database.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS fight_results (\n",
    "    EVENT TEXT,\n",
    "    BOUT TEXT,\n",
    "    OUTCOME TEXT,\n",
    "    WEIGHTCLASS TEXT,\n",
    "    METHOD TEXT,\n",
    "    ROUND INTEGER,\n",
    "    TIME TEXT,\n",
    "    TIME_FORMAT TEXT,\n",
    "    REFEREE TEXT,\n",
    "    DETAILS TEXT,\n",
    "    URL TEXT,\n",
    "    DATE TEXT,\n",
    "    LOCATION TEXT,\n",
    "    FIGHTER1 TEXT,\n",
    "    FIGHTER2 TEXT,\n",
    "    WINNING_FIGHTER TEXT\n",
    ")\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "latest_master_df.to_sql('fight_results', conn, if_exists='replace', index=False)\n",
    "\n",
    "result = cursor.execute(\"SELECT COUNT(*) FROM fight_results\").fetchone()[0]\n",
    "\n",
    "conn.close()\n",
    "\n",
    "result\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "master_df = pd.read_csv('./data/github/master.csv')\n",
    "fighter_info_df = pd.read_csv('./data-raw/fighter_info.csv')\n",
    "\n",
    "fighter_info_df['Fighter'] = fighter_info_df['Fighter'].str.strip().str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "master_df['FIGHTER1'] = master_df['FIGHTER1'].str.strip().str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "master_df['FIGHTER2'] = master_df['FIGHTER2'].str.strip().str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "master_df.to_csv('./data/github/master.csv', index=False)\n",
    "fighter_info_df.to_csv('./data-raw/fighter_info.csv', index=False)\n",
    "\n",
    "print(\"Whitespace and special characters removed, and cleaned datasets saved.\")\n",
    "\n",
    "master_df = pd.read_csv('./data/github/master.csv')\n",
    "fighter_info_df = pd.read_csv('./data-raw/fighter_info.csv')\n",
    "\n",
    "womens_fights_master = master_df[master_df['WEIGHTCLASS'].str.contains('Women', case=False, na=False)]\n",
    "female_fighters = set(womens_fights_master['FIGHTER1']).union(set(womens_fights_master['FIGHTER2']))\n",
    "cleaned_fighter_info_df = fighter_info_df[~fighter_info_df['Fighter'].isin(female_fighters)]\n",
    "cleaned_fighter_info_df.to_csv('./data-raw/fighter_info.csv', index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to ./data-raw/fighter_info.csv\")\n",
    "\n",
    "fighter_info_df = pd.read_csv('./data-raw/fighter_info.csv')\n",
    "print(f\"Number of rows in fighter_info.csv: {len(fighter_info_df)}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "master_df = pd.read_csv('./data/github/master.csv')\n",
    "\n",
    "print(f\"Number of rows before: {len(master_df)}\")\n",
    "\n",
    "cleaned_master_df = master_df[~master_df['WEIGHTCLASS'].str.contains('Women', case=False, na=False)]\n",
    "\n",
    "print(f\"Number of rows before: {len(cleaned_master_df)}\")\n",
    "\n",
    "cleaned_master_df.to_csv('./data/github/master.csv', index=False)\n",
    "\n",
    "print(\"Women's fights removed, and cleaned master.csv saved.\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "master_df = pd.read_csv('./data/github/master.csv')\n",
    "\n",
    "master_df['DATE'] = pd.to_datetime(master_df['DATE'], errors='coerce')\n",
    "\n",
    "cleaned_master_df = master_df[master_df['DATE'] >= '2005-01-01']\n",
    "\n",
    "cleaned_master_df.to_csv('./data/github/master.csv', index=False)\n",
    "\n",
    "print(\"Rows with events before 2010 removed, and cleaned master.csv saved.\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "master_df = pd.read_csv('./data/github/master.csv')\n",
    "\n",
    "print(f\"Number of rows before: {len(master_df)}\")\n",
    "\n",
    "filtered_df = master_df[~master_df['WEIGHTCLASS'].str.contains('Ultimate Fighter|TUF', case=False, na=False)]\n",
    "\n",
    "print(f\"Number of rows before: {len(filtered_df)}\")\n",
    "\n",
    "filtered_df.to_csv('./data/github/master.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084cd845",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
