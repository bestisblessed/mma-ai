{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: ./data/github/ufc_event_details.csv\n",
      "Downloaded: ./data/github/ufc_fight_results.csv\n",
      "Downloaded: ./data/github/ufc_fighter_tott.csv\n",
      "Total rows in ./data/github/ufc_event_details.csv: 735\n",
      "Total rows in ./data/github/ufc_fight_results.csv: 8165\n",
      "Total rows in ./data/github/ufc_fighter_tott.csv: 4376\n"
     ]
    }
   ],
   "source": [
    "# Download github data\n",
    "\n",
    "shutil.rmtree('./data')  # Remove the directory and all its contents\n",
    "os.mkdir('./data')\n",
    "os.makedirs('./data/github/', exist_ok=True)\n",
    "urls = [\n",
    "    'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/ufc_event_details.csv',\n",
    "    'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/ufc_fight_results.csv',\n",
    "    #'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/ufc_fight_details.csv',\n",
    "    #'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/ufc_fight_stats.csv',\n",
    "    #'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/ufc_fighter_details.csv',\n",
    "    'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/main/ufc_fighter_tott.csv'\n",
    "]\n",
    "for url in urls:\n",
    "    filename = os.path.join('./data/github/', url.split('/')[-1])\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download: {url}\")\n",
    "for url in urls:\n",
    "    filename = os.path.join('./data/github/', url.split('/')[-1])\n",
    "    df = pd.read_csv(filename)\n",
    "    print(f\"Total rows in {filename}: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4376 fighters from CSV\n",
      "Starting FAST parallel scraping with 10 threads...\n",
      "✓ Progress: 1/4376 - Latest: Ricardo Abreu\n",
      "✓ Progress: 2/4376 - Latest: Papy Abedi\n",
      "✓ Progress: 3/4376 - Latest: Tom Aaron\n",
      "✓ Progress: 4/4376 - Latest: Danny Abbadi\n",
      "✓ Progress: 5/4376 - Latest: David Abbott\n",
      "✓ Progress: 6/4376 - Latest: Hiroyuki Abe\n",
      "✓ Progress: 7/4376 - Latest: Daichi Abe\n",
      "✓ Progress: 8/4376 - Latest: Daniel Acacio\n",
      "✓ Progress: 9/4376 - Latest: Zarrukh Adashev\n",
      "✓ Progress: 10/4376 - Latest: Scott Adams\n",
      "✓ Progress: 11/4376 - Latest: Anthony Adams\n",
      "✓ Progress: 12/4376 - Latest: Klidson Abreu\n",
      "✓ Progress: 13/4376 - Latest: Shamil Abdurakhimov\n",
      "✓ Progress: 14/4376 - Latest: Juan Adams\n",
      "✓ Progress: 15/4376 - Latest: Israel Adesanya\n",
      "✓ Progress: 16/4376 - Latest: Sam Adkins\n",
      "✓ Progress: 17/4376 - Latest: Jessica Aguilar\n",
      "✓ Progress: 18/4376 - Latest: Mariya Agapova\n",
      "✓ Progress: 19/4376 - Latest: Nick Agallar\n",
      "✓ Progress: 20/4376 - Latest: Marcelo Aguiar\n",
      "✓ Progress: 100/4376 - Latest: Maiara Amanajas dos Santos\n",
      "✓ Progress: 200/4376 - Latest: Yohan Banks\n",
      "✓ Progress: 300/4376 - Latest: Jason Black\n",
      "✓ Progress: 400/4376 - Latest: Cody Brundage\n",
      "✓ Progress: 500/4376 - Latest: Cody Canterbury\n",
      "✓ Progress: 600/4376 - Latest: Dave Cochran\n",
      "✓ Progress: 700/4376 - Latest: Larry Cureton\n",
      "✓ Progress: 800/4376 - Latest: Ryan Diaz\n",
      "✓ Progress: 900/4376 - Latest: John Elam\n",
      "✓ Progress: 1000/4376 - Latest: Anthony Figueroa\n",
      "✓ Progress: 1100/4376 - Latest: Manvel Gamburyan\n",
      "✓ Progress: 1200/4376 - Latest: Jared Gooden\n",
      "✓ Progress: 1300/4376 - Latest: Jake Hadley\n",
      "✓ Progress: 1400/4376 - Latest: Brady Hiestand\n",
      "✓ Progress: 1500/4376 - Latest: Fabiano Iha\n",
      "✓ Progress: 1600/4376 - Latest: Brian Johnston\n",
      "✓ Progress: 1700/4376 - Latest: Alfred Khashakyan\n",
      "✓ Progress: 1800/4376 - Latest: Aaron Lanfranco\n",
      "✓ Progress: 1900/4376 - Latest: Jake Lindsey\n",
      "✓ Progress: 2000/4376 - Latest: Aliev Makhmud\n",
      "✓ Progress: 2100/4376 - Latest: Rob McCullough\n",
      "✓ Progress: 2200/4376 - Latest: Mario Miranda\n",
      "✓ Progress: 2300/4376 - Latest: Pedro Munhoz\n",
      "✓ Progress: 2400/4376 - Latest: Akiyo Nishiura\n",
      "✓ Progress: 2500/4376 - Latest: Larissa Pacheco\n",
      "✓ Progress: 2600/4376 - Latest: Mike Perry\n",
      "✓ Progress: 2700/4376 - Latest: Jose Quinonez\n",
      "✓ Progress: 2800/4376 - Latest: Dante Rivera\n",
      "✓ Progress: 2900/4376 - Latest: Amir Sadollah\n",
      "✓ Progress: 3000/4376 - Latest: Kerry Schall\n",
      "✓ Progress: 3100/4376 - Latest: Jay Silva\n",
      "✓ Progress: 3200/4376 - Latest: Charon Spain\n",
      "✓ Progress: 3300/4376 - Latest: Osamu Tachihikari\n",
      "✓ Progress: 3400/4376 - Latest: Salim Touahri\n",
      "✓ Progress: 3500/4376 - Latest: Bryan Vetell\n",
      "✓ Progress: 3600/4376 - Latest: Patrik White\n",
      "✓ Progress: 3700/4376 - Latest: Besam Yousef\n",
      "✓ Progress: 3800/4376 - Latest: Ho Taek Oh\n",
      "✓ Progress: 3900/4376 - Latest: William Gomis\n",
      "✓ Progress: 4000/4376 - Latest: Greg Velasco\n",
      "✓ Progress: 4100/4376 - Latest: Kiefer Crosbie\n",
      "✓ Progress: 4200/4376 - Latest: Jose Heraldo Souza\n",
      "✓ Progress: 4300/4376 - Latest: Daichi Kamiya\n",
      "\n",
      "=== FAST SCRAPING COMPLETED ===\n",
      "Total time: 195.6 seconds (3.3 minutes)\n",
      "Successfully scraped: 4371 fighters\n",
      "Failed: 5 fighters\n",
      "Success rate: 4371/4376 (99.9%)\n",
      "Average time per fighter: 0.045 seconds\n",
      "✓ Saved 4371 fighter details to ./data/fighter_info.csv\n",
      "=== SCRAPING SUMMARY ===\n",
      "Total fighters scraped: 4371\n",
      "Total columns: 15\n",
      "Success rate: 4371/4376 (99.9%)\n",
      "\n",
      "=== COLUMN ANALYSIS ===\n",
      "name: 0 missing (0.0%)\n",
      "url: 0 missing (0.0%)\n",
      "height: 367 missing (8.4%)\n",
      "weight: 86 missing (2.0%)\n",
      "reach: 1990 missing (45.5%)\n",
      "stance: 890 missing (20.4%)\n",
      "dob: 763 missing (17.5%)\n",
      "slpm: 0 missing (0.0%)\n",
      "str_acc: 0 missing (0.0%)\n",
      "sapm: 0 missing (0.0%)\n",
      "str_def: 0 missing (0.0%)\n",
      "td_avg: 0 missing (0.0%)\n",
      "td_acc: 0 missing (0.0%)\n",
      "td_def: 0 missing (0.0%)\n",
      "sub_avg: 0 missing (0.0%)\n",
      "\n",
      "=== SAMPLE DATA (First 3 fighters) ===\n",
      "            name                                                   url  height    weight reach    stance           dob  slpm str_acc  sapm str_def td_avg td_acc td_def sub_avg\n",
      "0  Ricardo Abreu  http://ufcstats.com/fighter-details/aa6e591c2a2cdecd  5' 11\"  185 lbs.        Orthodox  Apr 27, 1984  3.79     31%  3.98     68%   2.13    42%   100%     0.7\n",
      "1     Papy Abedi  http://ufcstats.com/fighter-details/c9f6385af6df66d7  5' 11\"  185 lbs.        Southpaw  Jun 30, 1978  2.80     55%  3.15     48%   3.47    57%    50%     1.3\n",
      "2      Tom Aaron  http://ufcstats.com/fighter-details/93fe7332d16c6ad9          155 lbs.                  Jul 13, 1978  0.00      0%  0.00      0%   0.00     0%     0%     0.0\n",
      "\n",
      "=== FAILED FIGHTERS ===\n",
      "- Jack Della: 500 Server Error: Internal Server Error for url: http://ufcstats.com/fighter-details/2c0dbe5db59013a7\n",
      "- Maimaiti Tuohati: 500 Server Error: Internal Server Error for url: http://ufcstats.com/fighter-details/31319179c9b1e6e7\n",
      "- Daniel Lacerda: 500 Server Error: Internal Server Error for url: http://ufcstats.com/fighter-details/b5681522d7d6b122\n",
      "- Jeka Saragih: 500 Server Error: Internal Server Error for url: http://ufcstats.com/fighter-details/8841c126a8ea629a\n",
      "- Stephen Erceg: 500 Server Error: Internal Server Error for url: http://ufcstats.com/fighter-details/9411103a865f32d3\n"
     ]
    }
   ],
   "source": [
    "# Scrape fighter pages from ufcstats.com\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "})\n",
    "df = pd.read_csv('./data/github/ufc_fighter_tott.csv')\n",
    "print(f\"Loaded {len(df)} fighters from CSV\")\n",
    "fighter_details = []\n",
    "failed_fighters = []\n",
    "details_lock = Lock()\n",
    "failed_lock = Lock()\n",
    "counter_lock = Lock()\n",
    "completed_count = 0\n",
    "def scrape_fighter(fighter_info):\n",
    "    \"\"\"Scrape a single fighter's details - thread-safe function\"\"\"\n",
    "    global completed_count\n",
    "    index, fighter_name, fighter_url = fighter_info\n",
    "    try:\n",
    "        response = session.get(fighter_url, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        fighter_data = {\n",
    "            'name': fighter_name,\n",
    "            'url': fighter_url,\n",
    "            'height': '', 'weight': '', 'reach': '', 'stance': '', 'dob': '',\n",
    "            'slpm': '', 'str_acc': '', 'sapm': '', 'str_def': '',\n",
    "            'td_avg': '', 'td_acc': '', 'td_def': '', 'sub_avg': ''\n",
    "        }\n",
    "        info_boxes = soup.find_all('div', class_='b-list__info-box')\n",
    "        for info_box in info_boxes:\n",
    "            list_items = info_box.find_all('li', class_='b-list__box-list-item')\n",
    "            for item in list_items:\n",
    "                title_elem = item.find('i', class_='b-list__box-item-title')\n",
    "                if title_elem:\n",
    "                    title = title_elem.text.strip().lower().replace(':', '')\n",
    "                    value = item.get_text().replace(title_elem.get_text(), '').strip()\n",
    "                    if 'height' in title:\n",
    "                        fighter_data['height'] = value\n",
    "                    elif 'weight' in title:\n",
    "                        fighter_data['weight'] = value\n",
    "                    elif 'reach' in title:\n",
    "                        fighter_data['reach'] = value\n",
    "                    elif 'stance' in title:\n",
    "                        fighter_data['stance'] = value\n",
    "                    elif 'dob' in title:\n",
    "                        fighter_data['dob'] = value\n",
    "                    elif 'slpm' in title:\n",
    "                        fighter_data['slpm'] = value\n",
    "                    elif 'str. acc' in title:\n",
    "                        fighter_data['str_acc'] = value\n",
    "                    elif 'sapm' in title:\n",
    "                        fighter_data['sapm'] = value\n",
    "                    elif 'str. def' in title:\n",
    "                        fighter_data['str_def'] = value\n",
    "                    elif 'td avg' in title:\n",
    "                        fighter_data['td_avg'] = value\n",
    "                    elif 'td acc' in title:\n",
    "                        fighter_data['td_acc'] = value\n",
    "                    elif 'td def' in title:\n",
    "                        fighter_data['td_def'] = value\n",
    "                    elif 'sub. avg' in title:\n",
    "                        fighter_data['sub_avg'] = value\n",
    "        for key, value in fighter_data.items():\n",
    "            if value == '--':\n",
    "                fighter_data[key] = ''\n",
    "        with details_lock:\n",
    "            fighter_details.append(fighter_data)\n",
    "        with counter_lock:\n",
    "            completed_count += 1\n",
    "            if completed_count % 100 == 0 or completed_count <= 20:\n",
    "                print(f\"✓ Progress: {completed_count}/{len(df)} - Latest: {fighter_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        with failed_lock:\n",
    "            failed_fighters.append({'name': fighter_name, 'url': fighter_url, 'error': str(e)})\n",
    "        with counter_lock:\n",
    "            completed_count += 1\n",
    "            if completed_count <= 20:  \n",
    "                print(f\"✗ Failed: {fighter_name} - {str(e)}\")\n",
    "        return False\n",
    "fighter_list = [(index, row['FIGHTER'], row['URL']) for index, row in df.iterrows()]\n",
    "print(f\"Starting FAST parallel scraping with 10 threads...\")\n",
    "start_time = time.time()\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(scrape_fighter, fighter_info) for fighter_info in fighter_list]\n",
    "    for future in as_completed(futures):\n",
    "        future.result()  \n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\n=== FAST SCRAPING COMPLETED ===\")\n",
    "print(f\"Total time: {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)\")\n",
    "print(f\"Successfully scraped: {len(fighter_details)} fighters\")\n",
    "print(f\"Failed: {len(failed_fighters)} fighters\")\n",
    "print(f\"Success rate: {len(fighter_details)}/{len(df)} ({len(fighter_details)/len(df)*100:.1f}%)\")\n",
    "print(f\"Average time per fighter: {elapsed_time/len(df):.3f} seconds\")\n",
    "if fighter_details:\n",
    "    fieldnames = [\n",
    "        'name', 'url', 'height', 'weight', 'reach', 'stance', 'dob',\n",
    "        'slpm', 'str_acc', 'sapm', 'str_def', 'td_avg', 'td_acc', 'td_def', 'sub_avg'\n",
    "    ]\n",
    "    with open('./data/fighter_info.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(fighter_details)\n",
    "    print(f\"✓ Saved {len(fighter_details)} fighter details to ./data/fighter_info.csv\")\n",
    "else:\n",
    "    print(\"No fighter details to save\")\n",
    "    \n",
    "# Display summary and sample data\n",
    "if fighter_details:\n",
    "    df_results = pd.DataFrame(fighter_details)\n",
    "    print(\"=== SCRAPING SUMMARY ===\")\n",
    "    print(f\"Total fighters scraped: {len(df_results)}\")\n",
    "    print(f\"Total columns: {len(df_results.columns)}\")\n",
    "    print(f\"Success rate: {len(fighter_details)}/{len(df)} ({len(fighter_details)/len(df)*100:.1f}%)\")\n",
    "    print(\"\\n=== COLUMN ANALYSIS ===\")\n",
    "    for col in df_results.columns:\n",
    "        missing_count = (df_results[col] == '').sum()\n",
    "        missing_pct = (missing_count / len(df_results)) * 100\n",
    "        print(f\"{col}: {missing_count} missing ({missing_pct:.1f}%)\")\n",
    "    print(f\"\\n=== SAMPLE DATA (First 3 fighters) ===\")\n",
    "    print(df_results.head(3).to_string())\n",
    "    if failed_fighters:\n",
    "        print(f\"\\n=== FAILED FIGHTERS ===\")\n",
    "        for fighter in failed_fighters:\n",
    "            print(f\"- {fighter['name']}: {fighter['error']}\")\n",
    "else:\n",
    "    print(\"No data scraped\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLEANING FIGHT RESULTS - REMOVING PRE-2010 EVENTS ===\n",
      "Original fight results count: 6900\n",
      "Events before 2010 to remove: 141\n",
      "Matching events found: 0\n",
      "❌ No matching events found - the fight results may already be filtered to post-2010 events\n",
      "=== FILTERING FIGHTER INFO TO MATCH POST-2010 FIGHTS ===\n",
      "Fight results count: 6900\n",
      "Fighter info count: 2085\n",
      "Unique fighters in post-2010 fights: 2134\n",
      "Fighters before filtering: 2085\n",
      "Fighters after filtering: 2085\n",
      "Fighters removed: 0\n",
      "✓ Saved filtered fighter info to ./data/fighter_info.csv\n",
      "Retention rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Delete all events before 2010 from ufc_fight_results.csv\n",
    "\n",
    "import pandas as pd\n",
    "print('=== CLEANING FIGHT RESULTS - REMOVING PRE-2010 EVENTS ===')\n",
    "fight_results_df = pd.read_csv('./data/github/ufc_fight_results.csv')\n",
    "events_df = pd.read_csv('./data/github/ufc_event_details.csv')\n",
    "print(f'Original fight results count: {len(fight_results_df)}')\n",
    "fight_results_df['EVENT'] = fight_results_df['EVENT'].str.strip()\n",
    "events_df['DATE_PARSED'] = pd.to_datetime(events_df['DATE'], format='%B %d, %Y')\n",
    "events_before_2010 = events_df[events_df['DATE_PARSED'] < '2010-01-01']\n",
    "event_names_to_remove = events_before_2010['EVENT'].tolist()\n",
    "print(f'Events before 2010 to remove: {len(event_names_to_remove)}')\n",
    "matching_events = set(event_names_to_remove) & set(fight_results_df['EVENT'].unique())\n",
    "print(f'Matching events found: {len(matching_events)}')\n",
    "if len(matching_events) > 0:\n",
    "    print(f'Removing fights from {len(matching_events)} events...')\n",
    "    fights_before_removal = len(fight_results_df)\n",
    "    fight_results_cleaned = fight_results_df[~fight_results_df['EVENT'].isin(matching_events)]\n",
    "    fights_after_removal = len(fight_results_cleaned)\n",
    "    print(f'Fights removed: {fights_before_removal - fights_after_removal}')\n",
    "    print(f'Remaining fights: {fights_after_removal}')\n",
    "    fight_results_cleaned.to_csv('./data/github/ufc_fight_results.csv', index=False)\n",
    "    print('✓ Saved cleaned fight results to ./data/github/ufc_fight_results.csv')\n",
    "    fight_results_df.to_csv('./data/github/ufc_fight_results_backup.csv', index=False)\n",
    "    print('✓ Created backup at ./data/github/ufc_fight_results_backup.csv')\n",
    "    fight_events_in_details = events_df[events_df['EVENT'].isin(fight_results_cleaned['EVENT'].unique())]\n",
    "    if len(fight_events_in_details) > 0:\n",
    "        fight_events_in_details['DATE_PARSED'] = pd.to_datetime(fight_events_in_details['DATE'], format='%B %d, %Y')\n",
    "        earliest = fight_events_in_details['DATE_PARSED'].min()\n",
    "        latest = fight_events_in_details['DATE_PARSED'].max()\n",
    "        print(f'New date range: {earliest.strftime(\"%Y-%m-%d\")} to {latest.strftime(\"%Y-%m-%d\")}')\n",
    "        before_2010_remaining = fight_events_in_details[fight_events_in_details['DATE_PARSED'] < '2010-01-01']\n",
    "        if len(before_2010_remaining) == 0:\n",
    "            print('✅ SUCCESS: All events before 2010 have been removed!')\n",
    "        else:\n",
    "            print(f'⚠️ WARNING: {len(before_2010_remaining)} events before 2010 still remain')\n",
    "else:\n",
    "    print('❌ No matching events found - the fight results may already be filtered to post-2010 events')\n",
    "    \n",
    "    \n",
    "\n",
    "# Delete all fighters before 2010 in fighter_info.csv\n",
    "print('=== FILTERING FIGHTER INFO TO MATCH POST-2010 FIGHTS ===')\n",
    "fight_results_df = pd.read_csv('./data/github/ufc_fight_results.csv')\n",
    "fighter_info_df = pd.read_csv('./data/fighter_info.csv')\n",
    "print(f'Fight results count: {len(fight_results_df)}')\n",
    "print(f'Fighter info count: {len(fighter_info_df)}')\n",
    "all_fighters_in_bouts = []\n",
    "for bout in fight_results_df['BOUT']:\n",
    "    if ' vs. ' in bout:\n",
    "        fighters = bout.split(' vs. ')\n",
    "        if len(fighters) == 2:\n",
    "            all_fighters_in_bouts.extend([fighters[0].strip(), fighters[1].strip()])\n",
    "unique_fighters_post_2010 = set(all_fighters_in_bouts)\n",
    "print(f'Unique fighters in post-2010 fights: {len(unique_fighters_post_2010)}')\n",
    "fighter_info_df['name_lower'] = fighter_info_df['name'].str.lower()\n",
    "unique_fighters_lower = {name.lower() for name in unique_fighters_post_2010}\n",
    "fighter_info_filtered = fighter_info_df[fighter_info_df['name_lower'].isin(unique_fighters_lower)]\n",
    "fighter_info_filtered = fighter_info_filtered.drop('name_lower', axis=1)\n",
    "print(f'Fighters before filtering: {len(fighter_info_df)}')\n",
    "print(f'Fighters after filtering: {len(fighter_info_filtered)}')\n",
    "print(f'Fighters removed: {len(fighter_info_df) - len(fighter_info_filtered)}')\n",
    "fighter_info_filtered.to_csv('./data/fighter_info.csv', index=False)\n",
    "print('✓ Saved filtered fighter info to ./data/fighter_info.csv')\n",
    "print(f'Retention rate: {len(fighter_info_filtered)/len(fighter_info_df)*100:.1f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
